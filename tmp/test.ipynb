{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ModelVerification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LazySets\n",
    "using PyCall\n",
    "using CSV\n",
    "using ONNX\n",
    "using Flux\n",
    "using Test\n",
    "using NNlib\n",
    "using ONNXNaiveNASflux\n",
    "using NaiveNASflux\n",
    "using Zygote\n",
    "# using DataFrames\n",
    "# import Flux: flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Flux: onehotbatch, onecold, flatten\n",
    "# using Flux.Losses: logitcrossentropy\n",
    "# using Statistics: mean\n",
    "using CUDA\n",
    "using MLDatasets: CIFAR10, MNIST\n",
    "using MLUtils: splitobs, DataLoader\n",
    "using Accessors\n",
    "using Profile\n",
    "using LinearAlgebra\n",
    "using Einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params([Float32[1.7481226;;], Float32[0.2518775]])\n",
      "Dense{typeof(identity), CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}\n"
     ]
    }
   ],
   "source": [
    "x = cu([1])\n",
    "y = cu([2])\n",
    "model = Dense(1, 1) \n",
    "model = fmap(cu, model)\n",
    "loss(x, y) = Flux.mse(x, y)\n",
    "optimizer = Flux.Optimiser(Flux.ADAM(0.1))\n",
    "opt_state = Flux.setup(optimizer, model)\n",
    "for i in 1 : 500\n",
    "    grads = Flux.gradient(model) do m\n",
    "        result = m(x) \n",
    "        loss(result, y)\n",
    "    end\n",
    "    Flux.update!(opt_state, model, grads[1])\n",
    "end\n",
    "println(Flux.params(model))\n",
    "println(typeof(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "x = rand(2, 1, 5)\n",
    "weight = rand(1, 2)\n",
    "ans = NNlib.batched_mul(x, weight)\n",
    "println(size(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params(["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]])\n",
      "Params([[20], [1]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.0], [0.0]]\n"
     ]
    }
   ],
   "source": [
    "struct AlphaLayer\n",
    "    node\n",
    "    alpha\n",
    "    lower\n",
    "    unstable_mask\n",
    "    lower_mask \n",
    "    upper_slope\n",
    "    lower_bias\n",
    "    upper_bias\n",
    "end\n",
    "Flux.@functor AlphaLayer (alpha,)\n",
    "\n",
    "function (f::AlphaLayer)(x)\n",
    "    Last_A = x[1]\n",
    "    Last_bias = x[2]\n",
    "    lower_slope = clamp.(f.alpha, 0, 1) .* f.unstable_mask .+ f.lower_mask \n",
    "    if f.lower \n",
    "        New_A = bound_oneside(Last_A, lower_slope, f.upper_slope)\n",
    "    else\n",
    "        New_A = bound_oneside(Last_A, f.upper_slope, lower_slope)\n",
    "    end\n",
    "\n",
    "    if isnothing(Last_bias)\n",
    "        return [New_A, nothing]\n",
    "    end\n",
    "    New_bias = multiply_bias(Last_bias, f.upper_slope, f.upper_bias, f.lower_bias)\n",
    "\n",
    "    return [New_A, New_bias]\n",
    "end\n",
    "\n",
    "#Upper bound slope and intercept according to CROWN relaxation.\n",
    "function relu_upper_bound(lower, upper)\n",
    "    lower_r = clamp.(lower, -Inf, 0)\n",
    "    upper_r = clamp.(upper, 0, Inf)\n",
    "    upper_r .= max.(upper_r, lower_r .+ 1e-8)\n",
    "    upper_slope = upper_r ./ (upper_r .- lower_r) #the slope of the relu upper bound\n",
    "    upper_bias = - lower_r .* upper_slope #the bias of the relu upper bound\n",
    "    return upper_slope, upper_bias\n",
    "end\n",
    "\n",
    "function clamp_mutiply_A(last_A, slope_pos, slope_neg) \n",
    "    A_pos = clamp.(last_A, 0, Inf)\n",
    "    A_neg = clamp.(last_A, -Inf, 0)\n",
    "    slope_pos = repeat(reshape(slope_pos,(1, size(slope_pos)...)), size(A_pos)[1], 1, 1) #add spec dim for slope_pos\n",
    "    slope_neg = repeat(reshape(slope_neg,(1, size(slope_neg)...)), size(A_neg)[1], 1, 1) #add spec dim for slope_pos\n",
    "    New_A = slope_pos .* A_pos .+ slope_neg .* A_neg \n",
    "    return New_A\n",
    "end \n",
    "\n",
    "\n",
    "function clamp_mutiply_bias(last_A, bias_pos, bias_neg) \n",
    "    A_pos = clamp.(last_A, 0, Inf)\n",
    "    A_neg = clamp.(last_A, -Inf, 0) \n",
    "    if bias_pos !== nothing #new_bias_pos = torch.einsum('s...b,s...b->sb', A_pos, bias_pos)\n",
    "        new_bias_pos = zeros((size(A_pos)[1], size(A_pos)[end]))#spec_dim x batch dim\n",
    "        @einsum new_bias_pos[s,b] = A_pos[s,r,b] * bias_pos[r,b]\n",
    "    end\n",
    "\n",
    "    if bias_neg !== nothing #new_bias_neg = torch.einsum('...sb,...sb->sb', A_neg, bias_neg)\n",
    "        new_bias_neg = zeros((size(A_neg)[1], size(A_neg)[end]))#spec_dim x batch dim\n",
    "        @einsum new_bias_neg[s,b] = A_neg[s,r,b] * bias_neg[r,b]\n",
    "    end\n",
    "    New_bias = new_bias_pos .+ new_bias_neg\n",
    "    return New_bias\n",
    "end \n",
    "\n",
    "#using last_A for getting New_A\n",
    "function multiply_by_A_signs(last_A, slope_pos, slope_neg)\n",
    "    if ndims(slope_pos) == 1\n",
    "        # Special case for LSTM, the bias term is 1-dimension. \n",
    "        New_A = clamp.(last_A, 0, Inf) .* slope_pos .+ clamp.(last_A, -Inf, 0) .* slope_neg\n",
    "    else\n",
    "        New_A = clamp_mutiply_A(last_A, slope_pos, slope_neg)\n",
    "        return New_A\n",
    "    end\n",
    "end\n",
    "\n",
    "function multiply_bias(last_A, upper_slope, bias_pos, bias_neg)\n",
    "    if ndims(upper_slope) == 1\n",
    "        # Special case for LSTM, the bias term is 1-dimension. \n",
    "        New_bias = clamp.(last_A, 0, Inf) .* bias_pos .+ clamp.(last_A, -Inf, 0) .* bias_neg\n",
    "    else\n",
    "        New_bias = clamp_mutiply_bias(last_A, bias_pos, bias_neg)\n",
    "        return New_bias\n",
    "    end\n",
    "end\n",
    "\n",
    "#bound oneside of the relu, like upper or lower\n",
    "function bound_oneside(last_A, slope_pos, slope_neg)\n",
    "    if isnothing(last_A)\n",
    "        return nothing, nothing\n",
    "    end\n",
    "    New_A = multiply_by_A_signs(last_A, slope_pos, slope_neg)\n",
    "    return New_A\n",
    "end\n",
    "\n",
    "\n",
    "alpha_lower = [20]\n",
    "alpha_upper = [1]\n",
    "unstable_mask = [1]\n",
    "lower_mask = [1]\n",
    "upper_slope = [2]\n",
    "upper_bias = [0]\n",
    "lower_bias = [0]\n",
    "lower = upper = true\n",
    "if lower == true\n",
    "    Alpha_Lower_Layer = AlphaLayer(\"relu_1\", alpha_lower, true, unstable_mask, lower_mask, upper_slope, upper_bias, lower_bias)\n",
    "end\n",
    "if upper ==true\n",
    "    Alpha_Upper_Layer = AlphaLayer(\"relu_1\", alpha_upper, false, unstable_mask, lower_mask, upper_slope, lower_bias, upper_bias)\n",
    "end\n",
    "a = []\n",
    "push!(a, Alpha_Lower_Layer) \n",
    "push!(a, Alpha_Upper_Layer)\n",
    "println(Flux.params(Alpha_Lower_Layer))\n",
    "println(Flux.params(a)) \n",
    "a = Chain(a)\n",
    "println(a([2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activation_node in model_info.activation_nodes\n",
    "    batch_info[activation_node][:split_active] = []\n",
    "end\n",
    "primals, duals, mini_inp = None, None, None\n",
    "upper_bound = zeros(size(lower_bound)) .+ Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu_3\n",
      "relu_2\n",
      "relu_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuArray{Float32, 3, CUDA.Mem.DeviceBuffer}\n",
      "\u001b[91m\u001b[1mError During Test\u001b[22m\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " at \u001b[39m\u001b[1m/home/verification/ModelVerification.jl/tmp/test.ipynb:22\u001b[22m\n",
      "  Test threw exception\n",
      "  Expression: (verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, comp_violated))).status == :violated\n",
      "  Compiling Tuple{typeof(ModelVerification.clamp_mutiply_bias), CuArray{Float32, 3, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}}: try/catch is not supported.\n",
      "  Refer to the Zygote documentation for fixes.\n",
      "  https://fluxml.ai/Zygote.jl/latest/limitations\n",
      "  \n",
      "  Stacktrace:\n",
      "    [1] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/Zygote/JeHtr/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface2.jl:101\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "    [2] \u001b[0m\u001b[1m_pullback\u001b[22m\u001b[0m\u001b[1m(\u001b[22m::\u001b[0mZygote.Context\u001b[90m{false}\u001b[39m, ::\u001b[0mtypeof(ModelVerification.clamp_mutiply_bias), ::\u001b[0mCuArray\u001b[90m{Float32, 3, CUDA.Mem.DeviceBuffer}\u001b[39m, ::\u001b[0mCuArray\u001b[90m{Float32, 2, CUDA.Mem.DeviceBuffer}\u001b[39m, ::\u001b[0mCuArray\u001b[90m{Float64, 2, CUDA.Mem.DeviceBuffer}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mZygote\u001b[39m \u001b[90m~/.julia/packages/Zygote/JeHtr/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface2.jl:101\u001b[24m\u001b[39m\n",
      "    [3] \u001b[0m\u001b[1m_pullback\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/ModelVerification.jl/src/propagate/operators/\u001b[39m\u001b[90m\u001b[4mrelu.jl:260\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "    [4] \u001b[0m\u001b[1m_pullback\u001b[22m\u001b[0m\u001b[1m(\u001b[22m::\u001b[0mZygote.Context\u001b[90m{false}\u001b[39m, ::\u001b[0mtypeof(ModelVerification.multiply_bias), ::\u001b[0mCuArray\u001b[90m{Float32, 3, CUDA.Mem.DeviceBuffer}\u001b[39m, ::\u001b[0mCuArray\u001b[90m{Float64, 2, CUDA.Mem.DeviceBuffer}\u001b[39m, ::\u001b[0mCuArray\u001b[90m{Float32, 2, CUDA.Mem.DeviceBuffer}\u001b[39m, ::\u001b[0mCuArray\u001b[90m{Float64, 2, CUDA.Mem.DeviceBuffer}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mZygote\u001b[39m \u001b[90m~/.julia/packages/Zygote/JeHtr/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface2.jl:0\u001b[24m\u001b[39m\n",
      "    [5] \u001b[0m\u001b[1m_pullback\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/ModelVerification.jl/src/propagate/operators/\u001b[39m\u001b[90m\u001b[4mrelu.jl:288\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "    [6] \u001b[0m\u001b[1m_pullback\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mctx\u001b[39m::\u001b[0mZygote.Context\u001b[90m{false}\u001b[39m, \u001b[90mf\u001b[39m::\u001b[0mModelVerification.AlphaLayer, \u001b[90margs\u001b[39m::\u001b[0mVector\u001b[90m{CuArray{Float32, N, CUDA.Mem.DeviceBuffer} where N}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mZygote\u001b[39m \u001b[90m~/.julia/packages/Zygote/JeHtr/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface2.jl:0\u001b[24m\u001b[39m\n",
      "    [7] \u001b[0m\u001b[1m_pullback\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/dev/Flux/src/layers/\u001b[39m\u001b[90m\u001b[4mbasic.jl:63\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "    [8] \u001b[0m\u001b[1m_pullback\u001b[22m\u001b[0m\u001b[1m(\u001b[22m::\u001b[0mZygote.Context\u001b[90m{false}\u001b[39m, ::\u001b[0mtypeof(Flux._applychain), ::\u001b[0mVector\u001b[90m{Any}\u001b[39m, ::\u001b[0mVector\u001b[90m{CuArray{Float32, N, CUDA.Mem.DeviceBuffer} where N}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mZygote\u001b[39m \u001b[90m~/.julia/packages/Zygote/JeHtr/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface2.jl:0\u001b[24m\u001b[39m\n",
      "    [9] \u001b[0m\u001b[1m_pullback\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/dev/Flux/src/layers/\u001b[39m\u001b[90m\u001b[4mbasic.jl:51\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "   [10] \u001b[0m\u001b[1m_pullback\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/ModelVerification.jl/src/propagate/\u001b[39m\u001b[90m\u001b[4mbound.jl:186\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "   [11] \u001b[0m\u001b[1m_pullback\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mctx\u001b[39m::\u001b[0mZygote.Context\u001b[90m{false}\u001b[39m, \u001b[90mf\u001b[39m::\u001b[0mModelVerification.var\"#56#57\"\u001b[90m{Vector{CuArray{Float32, N, CUDA.Mem.DeviceBuffer} where N}, ModelVerification.var\"#58#60\"}\u001b[39m, \u001b[90margs\u001b[39m::\u001b[0mChain\u001b[90m{Vector{Any}}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mZygote\u001b[39m \u001b[90m~/.julia/packages/Zygote/JeHtr/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface2.jl:0\u001b[24m\u001b[39m\n",
      "   [12] \u001b[0m\u001b[1mpullback\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mf\u001b[39m::\u001b[0mFunction, \u001b[90mcx\u001b[39m::\u001b[0mZygote.Context\u001b[90m{false}\u001b[39m, \u001b[90margs\u001b[39m::\u001b[0mChain\u001b[90m{Vector{Any}}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mZygote\u001b[39m \u001b[90m~/.julia/packages/Zygote/JeHtr/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface.jl:44\u001b[24m\u001b[39m\n",
      "   [13] \u001b[0m\u001b[1mpullback\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/Zygote/JeHtr/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface.jl:42\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "   [14] \u001b[0m\u001b[1mwithgradient\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mf\u001b[39m::\u001b[0mFunction, \u001b[90margs\u001b[39m::\u001b[0mChain\u001b[90m{Vector{Any}}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mZygote\u001b[39m \u001b[90m~/.julia/packages/Zygote/JeHtr/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface.jl:132\u001b[24m\u001b[39m\n",
      "   [15] \u001b[0m\u001b[1moptimize_bound\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mmodel\u001b[39m::\u001b[0mChain\u001b[90m{Vector{Any}}\u001b[39m, \u001b[90minput\u001b[39m::\u001b[0mVector\u001b[90m{CuArray{Float32, N, CUDA.Mem.DeviceBuffer} where N}\u001b[39m, \u001b[90mloss_func\u001b[39m::\u001b[0mModelVerification.var\"#58#60\", \u001b[90moptimizer\u001b[39m::\u001b[0mFlux.Optimise.Optimiser, \u001b[90mmax_iter\u001b[39m::\u001b[0mInt64\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[36mModelVerification\u001b[39m \u001b[90m~/ModelVerification.jl/src/propagate/\u001b[39m\u001b[90m\u001b[4mbound.jl:185\u001b[24m\u001b[39m\n",
      "   [16] \u001b[0m\u001b[1mprocess_bound\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mprop_method\u001b[39m::\u001b[0mAlphaCrown, \u001b[90mbatch_bound\u001b[39m::\u001b[0mModelVerification.AlphaCrownBound, \u001b[90mbatch_out_spec\u001b[39m::\u001b[0mModelVerification.LinearSpec, \u001b[90mbatch_info\u001b[39m::\u001b[0mDict\u001b[90m{Any, Any}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[36mModelVerification\u001b[39m \u001b[90m~/ModelVerification.jl/src/propagate/\u001b[39m\u001b[90m\u001b[4mbound.jl:213\u001b[24m\u001b[39m\n",
      "   [17] \u001b[0m\u001b[1msearch_branches\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90msearch_method\u001b[39m::\u001b[0mBFS, \u001b[90msplit_method\u001b[39m::\u001b[0mBisect, \u001b[90mprop_method\u001b[39m::\u001b[0mAlphaCrown, \u001b[90mproblem\u001b[39m::\u001b[0mProblem\u001b[90m{Hyperrectangle{Float64, Vector{Float64}, Vector{Float64}}, Complement{Float64, Hyperrectangle{Float64, Vector{Float64}, Vector{Float64}}}}\u001b[39m, \u001b[90mmodel_info\u001b[39m::\u001b[0mModelVerification.Model\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[36mModelVerification\u001b[39m \u001b[90m~/ModelVerification.jl/src/branching/\u001b[39m\u001b[90m\u001b[4msearch.jl:18\u001b[24m\u001b[39m\n",
      "   [18] \u001b[0m\u001b[1mverify\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/ModelVerification.jl/src/\u001b[39m\u001b[90m\u001b[4mModelVerification.jl:120\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "   [19] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/julia-1.8.5/share/julia/stdlib/v1.8/Test/src/\u001b[39m\u001b[90m\u001b[4mTest.jl:464\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "   [20] \u001b[0m\u001b[1mtest_mlp\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mprop_method\u001b[39m::\u001b[0mAlphaCrown\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[32mMain\u001b[39m \u001b[90m~/ModelVerification.jl/tmp/\u001b[39m\u001b[90m\u001b[4mtest.ipynb:22\u001b[24m\u001b[39m\n"
     ]
    },
    {
     "ename": "Test.FallbackTestSetException",
     "evalue": "Test.FallbackTestSetException(\"There was an error during testing\")",
     "output_type": "error",
     "traceback": [
      "Test.FallbackTestSetException(\"There was an error during testing\")\n",
      "\n",
      "Stacktrace:\n",
      " [1] record(ts::Test.FallbackTestSet, t::Union{Test.Error, Test.Fail})\n",
      "   @ Test ~/julia-1.8.5/share/julia/stdlib/v1.8/Test/src/Test.jl:946\n",
      " [2] do_test(result::Test.ExecutionResult, orig_expr::Any)\n",
      "   @ Test ~/julia-1.8.5/share/julia/stdlib/v1.8/Test/src/Test.jl:656\n",
      " [3] macro expansion\n",
      "   @ ~/julia-1.8.5/share/julia/stdlib/v1.8/Test/src/Test.jl:464 [inlined]\n",
      " [4] test_mlp(prop_method::AlphaCrown)\n",
      "   @ Main ~/ModelVerification.jl/tmp/test.ipynb:22\n",
      " [5] macro expansion\n",
      "   @ ~/ModelVerification.jl/tmp/test.ipynb:30 [inlined]\n",
      " [6] top-level scope\n",
      "   @ ./timing.jl:463"
     ]
    }
   ],
   "source": [
    "function test_mlp(prop_method)\n",
    "    small_nnet_file = \"/home/verification/ModelVerification.jl/test/networks/small_nnet.nnet\"\n",
    "    # small_nnet encodes the simple function 24*max(x + 1.5, 0) + 18.5\n",
    "    small_nnet = read_nnet(small_nnet_file, last_layer_activation = ModelVerification.ReLU())\n",
    "    flux_model = Flux.Chain(small_nnet)\n",
    "    #ONNXNaiveNASflux.save(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", flux_model, (1,1))\n",
    "    #println(flux_model)\n",
    "    #println(flux_model.layers[1].weight, \" \", flux_model.layers[1].bias) # max(x+1.5, 0) max(x+1.5, 0)              [0,4]\n",
    "    #println(flux_model.layers[2].weight, \" \", flux_model.layers[2].bias) # 4*max(x+1.5, 0)+2.5 4*max(x+1.5, 0)+2.5  [2.5, 18.5]\n",
    "    #println(flux_model.layers[3].weight, \" \", flux_model.layers[3].bias) # 24*max(x+1.5, 0)+18.5                    [18.5, 114.5]\n",
    "    in_hyper  = Hyperrectangle(low = [-2.5], high = [2.5]) # expected out: [18.5, 114.5]\n",
    "    out_violated    = Hyperrectangle(low = [19], high = [114]) # 20.0 ≤ y ≤ 90.0\n",
    "    out_holds = Hyperrectangle(low = [18], high = [115.0]) # -1.0 ≤ y ≤ 50.0\n",
    "    comp_violated    = Complement(Hyperrectangle(low = [10], high = [19])) # y ≤ 10.0 or 19 ≤ y\n",
    "    comp_holds    = Complement(Hyperrectangle(low = [115], high = [118])) # y ≤ 10.0 or 18 ≤ y\n",
    "    info = nothing\n",
    "    search_method = BFS(max_iter=100, batch_size=1)\n",
    "    split_method = Bisect(1)\n",
    "    #@test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, out_holds)).status == :holds\n",
    "    #@test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, out_violated)).status == :violated\n",
    "    #@test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, comp_holds)).status == :holds\n",
    "    @test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, comp_violated)).status == :violated\n",
    "    #= @test verify(search_method, split_method, prop_method, Problem(flux_model, in_hyper, out_holds)).status == :holds\n",
    "    @test verify(search_method, split_method, prop_method, Problem(flux_model, in_hyper, out_violated)).status == :violated\n",
    "    @test verify(search_method, split_method, prop_method, Problem(flux_model, in_hyper, comp_holds)).status == :holds\n",
    "    @test verify(search_method, split_method, prop_method, Problem(flux_model, in_hyper, comp_violated)).status == :violated =#\n",
    "end\n",
    "@timed begin\n",
    "    #for i in 1:1\n",
    "        test_mlp(AlphaCrown(Crown(true, true), true, false, Flux.Optimiser(Flux.ADAM(0.1)), 10))\n",
    "        #test_mlp(Ai2z())\n",
    "        #test_mlp(Crown(true, true))\n",
    "        #test_mlp(StarSet(Crown(true, true)))\n",
    "    #end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "(2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "a = [1.0; -1.0;;;]\n",
    "println(size(a))\n",
    "b = [18.5;;]\n",
    "println(size(b))\n",
    "c = batched_vec(a, b)\n",
    "println(size(c))\n",
    "d = batched_mul(a, b)\n",
    "println(size(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1)\n",
      "[18.5;;;]\n"
     ]
    }
   ],
   "source": [
    "last_A = [1;;;]\n",
    "println(size(last_A))\n",
    "x = [18.5;;]\n",
    "println(size(x))\n",
    "bias = [0;;]\n",
    "println(size(bias))\n",
    "out = NNlib.batched_mul(last_A, x) .+ bias\n",
    "println(size(out))\n",
    "println(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5; 1.5;;]\n",
      "(1, 1, 1)\n",
      "[28.799999999999997;;;]\n"
     ]
    }
   ],
   "source": [
    "last_A = [9.6 9.6;;;]\n",
    "bias = [1.5; 1.5;;]\n",
    "println(bias)\n",
    "New_bias = NNlib.batched_mul(last_A, bias)\n",
    "println(size(New_bias))\n",
    "println(New_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain([\n",
    "    Conv((3, 3), 3 => 8, relu, pad=SamePad(), stride=(2, 2)), #pad=SamePad() ensures size(output,d) == size(x,d) / stride.\n",
    "    BatchNorm(8),\n",
    "    MeanPool((2,2)),\n",
    "    SkipConnection(\n",
    "        Chain([\n",
    "            Conv((5, 5), 8 => 8, relu, pad=SamePad(), stride=(1, 1))\n",
    "            ]),\n",
    "        +\n",
    "    ),\n",
    "    #ConvTranspose((3, 3), 8 => 4, relu, pad=SamePad(), stride=(2, 2)),#pad=SamePad() ensures size(output,d) == size(x,d) * stride.\n",
    "    Flux.flatten,\n",
    "    Dense(512, 100, relu),\n",
    "    Dense(100, 10)\n",
    "])\n",
    "testmode!(model)\n",
    "# image_seeds = CIFAR10(:train)[1:5].features # 32 x 32 x 3 x 5\n",
    "image_seeds = [CIFAR10(:train)[i].features for i in 1:2]\n",
    "input_set = ImageConvexHull(image_seeds)\n",
    "# println(typeof(image_seeds[1][1,1,1,1]))\n",
    "search_method = BFS(max_iter=1, batch_size=1)\n",
    "split_method = Bisect(1)\n",
    "output_set = BallInf(zeros(10), 1.0)\n",
    "onnx_model_path = \"/home/verification/ModelVerification.jl/mlp.onnx\"\n",
    "flux_model = model\n",
    "image_shape = (32, 32, 3, 5)\n",
    "println(image_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStar()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(flux_model, input_set, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStarZono()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(onnx_model_path, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain([\n",
    "    Flux.flatten,\n",
    "    Dense(784, 200, relu),\n",
    "    Dense(200, 10)\n",
    "])\n",
    "image_seeds = [MNIST(:train)[i].features for i in 1:1]\n",
    "search_method = BFS(max_iter=1, batch_size=1)\n",
    "split_method = Bisect(1)\n",
    "output_set = BallInf(zeros(10), 1.0)\n",
    "onnx_model_path = \"/home/verification/ModelVerification.jl/debug.onnx\"\n",
    "Flux_model = model\n",
    "image_shape = (28, 28, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain([\n",
    "    Conv((3, 3), 3 => 8, relu, pad=SamePad(), stride=(2, 2)), #pad=SamePad() ensures size(output,d) == size(x,d) / stride.\n",
    "    BatchNorm(8),\n",
    "    MeanPool((2,2)),\n",
    "    SkipConnection(\n",
    "        Chain([\n",
    "            Conv((5, 5), 8 => 8, relu, pad=SamePad(), stride=(1, 1))\n",
    "            ]),\n",
    "        +\n",
    "    ),\n",
    "    Conv((3, 3), 8 => 8, relu, pad=SamePad(), stride=(2, 2)),\n",
    "    #ConvTranspose((3, 3), 8 => 4, relu, pad=SamePad(), stride=(2, 2)),#pad=SamePad() ensures size(output,d) == size(x,d) * stride.\n",
    "    Flux.flatten,\n",
    "    Dense(128, 100, relu),\n",
    "    Dense(100, 10)\n",
    "])\n",
    "testmode!(model)\n",
    "# image_seeds = CIFAR10(:train)[1:5].features # 32 x 32 x 3 x 5\n",
    "image_seeds = [CIFAR10(:train)[i].features for i in 1:2]\n",
    "# println(typeof(image_seeds[1][1,1,1,1]))\n",
    "search_method = BFS(max_iter=1, batch_size=1)\n",
    "split_method = Bisect(1)\n",
    "output_set = BallInf(zeros(10), 1.0)\n",
    "onnx_model_path = \"/home/verification/ModelVerification.jl/mlp.onnx\"\n",
    "Flux_model = model\n",
    "image_shape = (32, 32, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStar()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(onnx_model_path, Flux_model, image_shape, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStarZono()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(onnx_model_path, Flux_model, image_shape, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain([\n",
    "    Conv((3, 3), 3 => 128, relu, pad=SamePad(), stride=(2, 2)), #pad=SamePad() ensures size(output,d) == size(x,d) / stride.\n",
    "    BatchNorm(128),\n",
    "    MeanPool((2,2)),\n",
    "    SkipConnection(\n",
    "        Chain([\n",
    "            Conv((5, 5), 128 => 128, relu, pad=SamePad(), stride=(1, 1))\n",
    "            ]),\n",
    "        +\n",
    "    ),\n",
    "    ConvTranspose((3, 3), 128 => 128, relu, pad=SamePad(), stride=(2, 2)),#pad=SamePad() ensures size(output,d) == size(x,d) * stride.\n",
    "    Flux.flatten,\n",
    "    Dense(32768, 100, relu),\n",
    "    Dense(100, 10)\n",
    "])\n",
    "testmode!(model)\n",
    "# image_seeds = CIFAR10(:train)[1:5].features # 32 x 32 x 3 x 5\n",
    "image_seeds = [CIFAR10(:train)[i].features for i in 1:2]\n",
    "# println(typeof(image_seeds[1][1,1,1,1]))\n",
    "search_method = BFS(max_iter=1, batch_size=1)\n",
    "split_method = Bisect(1)\n",
    "output_set = BallInf(zeros(10), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStarZono()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(model, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profile.clear()\n",
    "@profile verify(search_method, split_method, prop_method, Problem(model, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"./prof.txt\", \"w\") do s\n",
    "    Profile.print(IOContext(s, :displaysize => (24, 500)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
