{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ModelVerification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LazySets\n",
    "using PyCall\n",
    "using CSV\n",
    "using ONNX\n",
    "using Flux\n",
    "using Test\n",
    "using NNlib\n",
    "using ONNXNaiveNASflux\n",
    "using NaiveNASflux\n",
    "using Zygote\n",
    "# using DataFrames\n",
    "# import Flux: flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Flux: onehotbatch, onecold, flatten\n",
    "# using Flux.Losses: logitcrossentropy\n",
    "# using Statistics: mean\n",
    "using CUDA\n",
    "using MLDatasets: CIFAR10, MNIST\n",
    "using MLUtils: splitobs, DataLoader\n",
    "using Accessors\n",
    "using Profile\n",
    "using LinearAlgebra\n",
    "using Einsum\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a = [-2.0; -4.0;;; -4.0; -8.0], b = [-1.0; -2.0;;; -1.0; -2.0])\n",
      "10.0\n",
      "Params([[0.1; 0.1;;; 0.1; 0.1], [0.1; 0.1;;; 0.1; 0.1]])\n",
      "(a = [-1.7; -3.7;;; -3.0; -7.0], b = [-0.85; -1.85;;; -0.75; -1.75])\n",
      "7.77\n",
      "Params([[0.19925155; 0.19972123;;; 0.1982575; 0.19943172], [0.19925155; 0.19972123;;; 0.1982575; 0.19943172]])\n",
      "(a = [-1.4022453; -3.4008362;;; -2.017425; -6.005683], b = [-0.70112264; -1.7004181;;; -0.50435627; -1.5014207])\n",
      "5.891634\n",
      "Params([[0.2970295; 0.29894242;;; 0.29260504; 0.29777992], [0.2970295; 0.29894242;;; 0.29260504; 0.29777992]])\n",
      "(a = [-1.1089115; -3.1031728;;; -1.0739496; -5.0222006], b = [-0.55445576; -1.5515864;;; -0.2684874; -1.2555501])\n",
      "4.363333\n",
      "Params([[0.39238304; 0.39741224;;; 0.37976182; 0.39439976], [0.39238304; 0.39741224;;; 0.37976182; 0.39439976]])\n",
      "(a = [-0.82285094; -2.8077633;;; -0.20238185; -4.0560026], b = [-0.41142547; -1.4038817;;; -0.050595462; -1.0140007])\n",
      "3.1709118\n",
      "Params([[0.4840957; 0.49484667;;; 0.4553892; 0.4884957], [0.4840957; 0.49484667;;; 0.4553892; 0.4884957]])\n"
     ]
    }
   ],
   "source": [
    "struct Layer\n",
    "    a\n",
    "    b\n",
    "end\n",
    "Flux.@functor Layer (a, b,)\n",
    "\n",
    "function (f::Layer)(x)\n",
    "    return NNlib.batched_mul(f.a, x) .+ f.b\n",
    "end\n",
    "a = cu(zeros(2, 1, 2))\n",
    "b = cu(zeros(2, 1, 2))\n",
    "x = cu([2;;; 4])\n",
    "y = cu([2; 4;;; 2; 4])\n",
    "model = fmap(cu, Layer(a, b))\n",
    "#println(Flux.params(model))\n",
    "loss(x, y) = Flux.mse(x, y)\n",
    "optimizer = Flux.Optimiser(Flux.ADAM(0.1))\n",
    "opt_state = Flux.setup(optimizer, model)\n",
    "for i in 1 : 5\n",
    "    losses, grads = Flux.withgradient(model) do m\n",
    "        result = m(x) \n",
    "        loss(result, y)\n",
    "    end\n",
    "    Flux.update!(opt_state, model, grads[1])\n",
    "    println(grads[1])\n",
    "    println(losses)\n",
    "    println(Flux.params(model)) \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7043097853266815"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ";;; 0.4922288426770137]\n",
      "(2, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "a = rand(2, 1, 2)\n",
    "b = rand(1, 1, 2)\n",
    "println(b)\n",
    "println(size(NNlib.batched_mul(a, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "AssertionError: radius must not be negative",
     "output_type": "error",
     "traceback": [
      "AssertionError: radius must not be negative\n",
      "\n",
      "Stacktrace:\n",
      " [1] macro expansion\n",
      "   @ ~/.julia/packages/LazySets/HlinV/src/Assertions/Assertions.jl:23 [inlined]\n",
      " [2] Hyperrectangle(center::Vector{Float64}, radius::Vector{Float64}; check_bounds::Bool)\n",
      "   @ LazySets ~/.julia/packages/LazySets/HlinV/src/Sets/Hyperrectangle.jl:91\n",
      " [3] Hyperrectangle(; high::Vector{Float64}, low::Vector{Int64}, check_bounds::Bool)\n",
      "   @ LazySets ~/.julia/packages/LazySets/HlinV/src/Sets/Hyperrectangle.jl:107\n",
      " [4] test_mlp(prop_method::BetaCrown)\n",
      "   @ Main ~/ModelVerification.jl/tmp/test.ipynb:12\n",
      " [5] macro expansion\n",
      "   @ ~/ModelVerification.jl/tmp/test.ipynb:26 [inlined]\n",
      " [6] top-level scope\n",
      "   @ ./timing.jl:463"
     ]
    }
   ],
   "source": [
    "function test_mlp(prop_method)\n",
    "    small_nnet_file = \"/home/verification/ModelVerification.jl/test/networks/small_nnet.nnet\"\n",
    "    # small_nnet encodes the simple function 24*max(x + 1.5, 0) + 18.5\n",
    "    small_nnet = read_nnet(small_nnet_file, last_layer_activation = ModelVerification.ReLU())\n",
    "    flux_model = Flux.Chain(small_nnet)\n",
    "    #ONNXNaiveNASflux.save(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", flux_model, (1,1))\n",
    "    #println(flux_model)\n",
    "    #println(flux_model.layers[1].weight, \" \", flux_model.layers[1].bias) # max(x+1.5, 0) max(x+1.5, 0)              [0,4]\n",
    "    #println(flux_model.layers[2].weight, \" \", flux_model.layers[2].bias) # 4*max(x+1.5, 0)+2.5 4*max(x+1.5, 0)+2.5  [2.5, 18.5]\n",
    "    #println(flux_model.layers[3].weight, \" \", flux_model.layers[3].bias) # 24*max(x+1.5, 0)+18.5                    [18.5, 114.5]\n",
    "    in_hyper  = Hyperrectangle(low = [-2.5], high = [2.5]) # expected out: [18.5, 114.5]\n",
    "    out_violated    = Hyperrectangle(low = [19])#, high = [114]) # 20.0 ≤ y ≤ 90.0\n",
    "    out_holds = Hyperrectangle(low = [18], high = [115.0]) # -1.0 ≤ y ≤ 50.0\n",
    "    comp_violated    = Complement(Hyperrectangle(low = [10], high = [19])) # y ≤ 10.0 or 19 ≤ y\n",
    "    comp_holds    = Complement(Hyperrectangle(low = [115], high = [118])) # y ≤ 10.0 or 18 ≤ y\n",
    "    info = nothing\n",
    "    search_method = BFS(max_iter=3, batch_size=2)\n",
    "    split_method = BaBSR(2)\n",
    "    #@test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, out_holds)).status == :holds\n",
    "    @test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, out_violated)).status == :violated\n",
    "    #@test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, comp_holds)).status == :holds\n",
    "    #@test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, comp_violated)).status == :violated\n",
    "end\n",
    "@timed begin\n",
    "    #for i in 1:1\n",
    "        test_mlp(BetaCrown(false, 2, Crown(false, true, true), true, false, Flux.Optimiser(Flux.ADAM(0.1)), 3))\n",
    "        #test_mlp(Ai2z())\n",
    "        #test_mlp(Crown(true, true))\n",
    "        #test_mlp(StarSet(Crown(true, true)))\n",
    "    #end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "1\n",
      "Any[Hyperrectangle{Float64, Vector{Float64}, Vector{Float64}}([0.0], [2.5])]\n",
      "relu_3\n",
      "[0.0;;]\n",
      "relu_2\n",
      "[0.0; 0.0;;]\n",
      "relu_1\n",
      "[0.7000000002500008; 0.7000000002500008;;]\n",
      "2\n",
      "3\n",
      "Any[Hyperrectangle{Float64, Vector{Float64}, Vector{Float64}}([-1.25], [1.25]), Hyperrectangle{Float64, Vector{Float64}, Vector{Float64}}([1.25], [1.25])]\n",
      "relu_3\n",
      "[0.0 0.0]\n",
      "relu_2\n",
      "[0.0 0.0; 0.0 0.0]\n",
      "relu_1\n",
      "[0.7000000002500008 0.0; 0.7000000002500008 0.0]\n",
      "\u001b[91m\u001b[1mTest Failed\u001b[22m\u001b[39m at \u001b[39m\u001b[1m/home/verification/ModelVerification.jl/tmp/test.ipynb:20\u001b[22m\n",
      "  Expression: (verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, out_violated))).status == :violated\n",
      "   Evaluated: unknown == violated\n"
     ]
    },
    {
     "ename": "Test.FallbackTestSetException",
     "evalue": "Test.FallbackTestSetException(\"There was an error during testing\")",
     "output_type": "error",
     "traceback": [
      "Test.FallbackTestSetException(\"There was an error during testing\")\n",
      "\n",
      "Stacktrace:\n",
      " [1] record(ts::Test.FallbackTestSet, t::Union{Test.Error, Test.Fail})\n",
      "   @ Test ~/julia-1.8.5/share/julia/stdlib/v1.8/Test/src/Test.jl:946\n",
      " [2] do_test(result::Test.ExecutionResult, orig_expr::Any)\n",
      "   @ Test ~/julia-1.8.5/share/julia/stdlib/v1.8/Test/src/Test.jl:656\n",
      " [3] macro expansion\n",
      "   @ ~/julia-1.8.5/share/julia/stdlib/v1.8/Test/src/Test.jl:464 [inlined]\n",
      " [4] test_mlp(prop_method::AlphaCrown)\n",
      "   @ Main ~/ModelVerification.jl/tmp/test.ipynb:20\n",
      " [5] macro expansion\n",
      "   @ ~/ModelVerification.jl/tmp/test.ipynb:30 [inlined]\n",
      " [6] top-level scope\n",
      "   @ ./timing.jl:463"
     ]
    }
   ],
   "source": [
    "function test_mlp(prop_method)\n",
    "    small_nnet_file = \"/home/verification/ModelVerification.jl/test/networks/small_nnet.nnet\"\n",
    "    # small_nnet encodes the simple function 24*max(x + 1.5, 0) + 18.5\n",
    "    small_nnet = read_nnet(small_nnet_file, last_layer_activation = ModelVerification.ReLU())\n",
    "    flux_model = Flux.Chain(small_nnet)\n",
    "    #ONNXNaiveNASflux.save(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", flux_model, (1,1))\n",
    "    #println(flux_model)\n",
    "    #println(flux_model.layers[1].weight, \" \", flux_model.layers[1].bias) # max(x+1.5, 0) max(x+1.5, 0)              [0,4]\n",
    "    #println(flux_model.layers[2].weight, \" \", flux_model.layers[2].bias) # 4*max(x+1.5, 0)+2.5 4*max(x+1.5, 0)+2.5  [2.5, 18.5]\n",
    "    #println(flux_model.layers[3].weight, \" \", flux_model.layers[3].bias) # 24*max(x+1.5, 0)+18.5                    [18.5, 114.5]\n",
    "    in_hyper  = Hyperrectangle(low = [-2.5], high = [2.5]) # expected out: [18.5, 114.5]\n",
    "    out_violated    = Hyperrectangle(low = [19], high = [114]) # 20.0 ≤ y ≤ 90.0\n",
    "    out_holds = Hyperrectangle(low = [18], high = [115.0]) # -1.0 ≤ y ≤ 50.0\n",
    "    comp_violated    = Complement(Hyperrectangle(low = [10], high = [19])) # y ≤ 10.0 or 19 ≤ y\n",
    "    comp_holds    = Complement(Hyperrectangle(low = [115], high = [118])) # y ≤ 10.0 or 18 ≤ y\n",
    "    info = nothing\n",
    "    search_method = BFS(max_iter=3, batch_size=2)\n",
    "    split_method = Bisect(1)\n",
    "    #@test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, out_holds)).status == :holds\n",
    "    @test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, out_violated)).status == :violated\n",
    "    #@test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, comp_holds)).status == :holds\n",
    "    #@test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, comp_violated)).status == :violated\n",
    "    #= @test verify(search_method, split_method, prop_method, Problem(flux_model, in_hyper, out_holds)).status == :holds\n",
    "    @test verify(search_method, split_method, prop_method, Problem(flux_model, in_hyper, out_violated)).status == :violated\n",
    "    @test verify(search_method, split_method, prop_method, Problem(flux_model, in_hyper, comp_holds)).status == :holds\n",
    "    @test verify(search_method, split_method, prop_method, Problem(flux_model, in_hyper, comp_violated)).status == :violated =#\n",
    "end\n",
    "@timed begin\n",
    "    #for i in 1:1\n",
    "        test_mlp(AlphaCrown(false, Crown(false, true, true), true, false, Flux.Optimiser(Flux.ADAM(0.1)), 3))\n",
    "        #test_mlp(Ai2z())\n",
    "        #test_mlp(Crown(true, true))\n",
    "        #test_mlp(StarSet(Crown(true, true)))\n",
    "    #end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4, 7, 9, 11, 12, 13, 19, 20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 6, 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[0.5581055353150743, 0.9204138604104602, 0.49169644895013387, 0.5819823567683335, 0.7841737348642372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5581055353150743, 0.0, 0.9204138604104602, 0.0, 0.49169644895013387, 0.5819823567683335, 0.7841737348642372, 0.0, 0.0, 0.0]\n",
      "("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5)\n",
      "(2, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5581055353150743 0.9204138604104602 0.49169644895013387 0.7841737348642372 0.0; 0.0 0.0 0.5819823567683335 0.0 0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0 0.0 0.0 0.0 1.0; 1.0 1.0 0.0 1.0 1.0]\n"
     ]
    }
   ],
   "source": [
    "a = [0.5284647839096102 0.9466078796040489; 0.2079903093502885 0.19692774736051; 0.8221945547831693 0.3318526763622769]\n",
    "b = [0.5581055353150743 0.9204138604104602 0.49169644895013387 0.7841737348642372 0.3046794102208209; 0.3340520423890394 0.14292402232326062 0.5819823567683335 0.4363631211893606 0.21381568166361997]\n",
    "c = [0.19615167803663092; 0.4362831955997829; 0.7907563085565711; 0.5431047673653794;;]\n",
    "score = [a, b, c]\n",
    "vec_score = []\n",
    "relu_node_neurons_range = []\n",
    "split_neurons_in_node = []\n",
    "split_neurons_index_in_node = []\n",
    "k = 10\n",
    "current_neuron_index = 1\n",
    "for matrix in score # matrix store neurons\n",
    "    vec_matrix = vec(matrix)# all neurons need to be flattened into a vector \n",
    "    vec_score = vcat(vec_score, vec_matrix)\n",
    "    push!(relu_node_neurons_range, [current_neuron_index, current_neuron_index + length(vec_matrix) - 1])\n",
    "    current_neuron_index += length(vec_matrix)\n",
    "end\n",
    "topk_index = partialsortperm(vec_score, 1:k, rev = true)\n",
    "topk_index = sort!(topk_index)\n",
    "println(topk_index)\n",
    "current_relu_node_neurons_range = relu_node_neurons_range[2]\n",
    "current_relu_node_split_neurons_index = topk_index[(topk_index .>= current_relu_node_neurons_range[1]) .& (topk_index .<= current_relu_node_neurons_range[2])]\n",
    "split_neurons_index_in_node = current_relu_node_split_neurons_index .- current_relu_node_neurons_range[1] .+ 1\n",
    "println(split_neurons_index_in_node)\n",
    "println(vec_score[current_relu_node_split_neurons_index])\n",
    "\n",
    "split_neurons_in_node = zeros(size(vec(b)))\n",
    "split_neurons_in_node[split_neurons_index_in_node] = b[split_neurons_index_in_node]\n",
    "println(split_neurons_in_node)\n",
    "println(size(split_neurons_index_in_node))\n",
    "split_neurons_in_node = reshape(split_neurons_in_node, size(b))\n",
    "println(size(b))\n",
    "println(size(split_neurons_in_node))\n",
    "println(split_neurons_in_node)\n",
    "\n",
    "neg_split_neurons_in_node = ones(size(vec(b)))\n",
    "neg_split_neurons_in_node[split_neurons_index_in_node] .= 0\n",
    "neg_split_neurons_in_node = reshape(neg_split_neurons_in_node, size(b))\n",
    "println(neg_split_neurons_in_node)\n",
    "#splited_neurons_mask .*= split_neurons_in_node\n",
    "#batch_info[node][splited_neurons_mask] = splited_neurons_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08754564 0.2532909 0.0570138 0.0309838 0.09425839999999999 0.24900168 0.21692964 0.0656158 0.64241074 0.05683992;;; 0.02090088 0.25883952000000005 0.07066605000000001 0.1266686 0.023623599999999998 0.18000204 0.23194958000000002 0.0661486 0.22239152 0.05627592]\n",
      "(1, 10, 2)\n",
      "(10, 2)\n",
      "[0.0 0.0; 0.0 0.0; 0.0 0.0; 0.0 0.0; 0.094022754 0.0; 0.0 0.0; 0.0 0.007538361350000008; 0.02440907760000001 0.0; 0.0 0.12453925120000002; 0.02299174764 0.0]\n"
     ]
    }
   ],
   "source": [
    "function relu_upper_bound(lower, upper)\n",
    "    lower_r = clamp.(lower, -Inf, 0)\n",
    "    upper_r = clamp.(upper, 0, Inf)\n",
    "    upper_r .= max.(upper_r, lower_r .+ 1e-8)\n",
    "    upper_slope = upper_r ./ (upper_r .- lower_r) #the slope of the relu upper bound\n",
    "    upper_bias = - lower_r .* upper_slope #the bias of the relu upper bound\n",
    "    return upper_slope, upper_bias\n",
    "end\n",
    "\n",
    "lower = [0.0733 -0.2387; -0.4172 0.4561; 0.4147 -0.2835; 0.2980 -0.4698; -0.0005 0.2590; 0.2553 0.0367; -0.2080 -0.1935; -0.1256 0.3723; 0.4952 -0.0880; -0.1191 -0.2540]\n",
    "A = [0.8017 0.7030 0.6828 0.1961 0.3994 0.3634 0.7698 0.8867 0.9313 0.5039;;; 0.1914 0.7184 0.8463 0.8017 0.1001 0.2627 0.8231 0.8939 0.3224 0.4989]\n",
    "upper = lower .+ 0.2\n",
    "unstable_mask = (upper .> 0) .& (lower .< 0)\n",
    "unstable_mask = reshape(unstable_mask, (1, size(unstable_mask)...))\n",
    "upper_slope, upper_bias = relu_upper_bound(lower, upper)\n",
    "intercept_temp = clamp.(A, -Inf, 0)\n",
    "intercept_candidate = intercept_temp .* reshape(upper_bias, (1, size(upper_bias)...))\n",
    "b_temp = [0.1092, 0.3603, 0.0835, 0.1580, 0.2360, 0.6852, 0.2818, 0.0740, 0.6898, 0.1128]\n",
    "b_temp = reshape(b_temp, (1, size(b_temp)...)) .* A\n",
    "println(b_temp)\n",
    "println(size(b_temp))\n",
    "upper_slope = reshape(upper_slope, (1, size(upper_slope)...))\n",
    "bias_candidate_1 = b_temp .* (upper_slope .- 1)\n",
    "bias_candidate_2 = b_temp .* upper_slope\n",
    "bias_candidate = max.(bias_candidate_1, bias_candidate_2)\n",
    "score_candidate = bias_candidate .+ intercept_candidate\n",
    "score_candidate = dropdims(mean((abs.(score_candidate) .* unstable_mask), dims = 1), dims = 1)\n",
    "println(size(score_candidate))\n",
    "println(score_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function relu_upper_bound(lower, upper)\n",
    "    lower_r = clamp.(lower, -Inf, 0)\n",
    "    upper_r = clamp.(upper, 0, Inf)\n",
    "    upper_r .= max.(upper_r, lower_r .+ 1e-8)\n",
    "    upper_slope = upper_r ./ (upper_r .- lower_r) #the slope of the relu upper bound\n",
    "    upper_bias = - lower_r .* upper_slope #the bias of the relu upper bound\n",
    "    return upper_slope, upper_bias\n",
    "end\n",
    "\n",
    "function branching_scores_kfsb(A, model_info, batch_info)\n",
    "    score = []\n",
    "    for node in reverse(model_info.activation_nodes)\n",
    "        layer = model_info.node_layer[node]\n",
    "        unstable_mask = batch_info[node][:unstable_mask]\n",
    "        unstable_mask = reshape(unstable_mask, (1, size(unstable_mask)...))\n",
    "        lower = batch_info[node][:pre_lower]\n",
    "        upper = batch_info[node][:pre_upper]\n",
    "        upper_slope, upper_bias = relu_upper_bound(lower, upper)\n",
    "\n",
    "        intercept_temp = clamp.(A, -Inf, 0)\n",
    "        intercept_candidate = intercept_temp .* reshape(upper_bias, (1, size(upper_bias)...))\n",
    "\n",
    "        input_node = model_info.pre_layer[node][1]\n",
    "        input_layer = model_info.node_layer[input_node]\n",
    "        if isa(layer, Flux.Conv)\n",
    "            if !isnothing(input_layer.bias)\n",
    "                b_temp = input_layer.bias\n",
    "            else\n",
    "                b_temp = 0\n",
    "            end\n",
    "        elseif isa(layer, Flux.Dense)\n",
    "            if !isnothing(input_layer.bias)\n",
    "                b_temp = input_layer.bias\n",
    "            else\n",
    "                b_temp = 0\n",
    "            end\n",
    "        elseif isa(layer, +)\n",
    "            b_temp = 0\n",
    "            for l in model_info.pre_layer[input_node]\n",
    "                l_layer = model_info.node_layer[l]\n",
    "                if isa(layer, Flux.Conv)\n",
    "                    if length(l_layer.inputs) > 2\n",
    "                        b_temp += typeof.bias\n",
    "                    end\n",
    "                end\n",
    "                if isa(layer, Flux.normalise)\n",
    "                    b_temp += 0\n",
    "                end\n",
    "                if isa(layer, +)\n",
    "                    for ll in model_info.pre_layer[l]\n",
    "                        ll_layer = model_info.node_layer[ll]\n",
    "                        if isa(layer, Flux.Conv)\n",
    "                            b_temp += ll_layer.bias\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        else\n",
    "            b_temp = 0\n",
    "        end\n",
    "        \n",
    "        b_temp = reshape(b_temp, (1, size(b_temp)...)) .* A\n",
    "        bias_candidate_1 = b_temp .* (upper_slope .- 1)\n",
    "        bias_candidate_2 = b_temp .* upper_slope\n",
    "        bias_candidate = max.(bias_candidate_1, bias_candidate_2)\n",
    "        score_candidate = bias_candidate .+ intercept_candidate\n",
    "        score_candidate = dropdims(mean((abs.(score_candidate) .* unstable_mask), dims = 1), dims = 1)\n",
    "        push!(score, score_candidate)\n",
    "    end\n",
    "    return score\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "(2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "a = [1.0; -1.0;;;]\n",
    "println(size(a))\n",
    "b = [18.5;;]\n",
    "println(size(b))\n",
    "c = batched_vec(a, b)\n",
    "println(size(c))\n",
    "d = batched_mul(a, b)\n",
    "println(size(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1)\n",
      "[18.5;;;]\n"
     ]
    }
   ],
   "source": [
    "last_A = [1;;;]\n",
    "println(size(last_A))\n",
    "x = [18.5;;]\n",
    "println(size(x))\n",
    "bias = [0;;]\n",
    "println(size(bias))\n",
    "out = NNlib.batched_mul(last_A, x) .+ bias\n",
    "println(size(out))\n",
    "println(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5; 1.5;;]\n",
      "(1, 1, 1)\n",
      "[28.799999999999997;;;]\n"
     ]
    }
   ],
   "source": [
    "last_A = [9.6 9.6;;;]\n",
    "bias = [1.5; 1.5;;]\n",
    "println(bias)\n",
    "New_bias = NNlib.batched_mul(last_A, bias)\n",
    "println(size(New_bias))\n",
    "println(New_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain([\n",
    "    Conv((3, 3), 3 => 8, relu, pad=SamePad(), stride=(2, 2)), #pad=SamePad() ensures size(output,d) == size(x,d) / stride.\n",
    "    BatchNorm(8),\n",
    "    MeanPool((2,2)),\n",
    "    SkipConnection(\n",
    "        Chain([\n",
    "            Conv((5, 5), 8 => 8, relu, pad=SamePad(), stride=(1, 1))\n",
    "            ]),\n",
    "        +\n",
    "    ),\n",
    "    #ConvTranspose((3, 3), 8 => 4, relu, pad=SamePad(), stride=(2, 2)),#pad=SamePad() ensures size(output,d) == size(x,d) * stride.\n",
    "    Flux.flatten,\n",
    "    Dense(512, 100, relu),\n",
    "    Dense(100, 10)\n",
    "])\n",
    "testmode!(model)\n",
    "# image_seeds = CIFAR10(:train)[1:5].features # 32 x 32 x 3 x 5\n",
    "image_seeds = [CIFAR10(:train)[i].features for i in 1:2]\n",
    "input_set = ImageConvexHull(image_seeds)\n",
    "# println(typeof(image_seeds[1][1,1,1,1]))\n",
    "search_method = BFS(max_iter=1, batch_size=1)\n",
    "split_method = Bisect(1)\n",
    "output_set = BallInf(zeros(10), 1.0)\n",
    "onnx_model_path = \"/home/verification/ModelVerification.jl/mlp.onnx\"\n",
    "flux_model = model\n",
    "image_shape = (32, 32, 3, 5)\n",
    "println(image_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStar()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(flux_model, input_set, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStarZono()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(onnx_model_path, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain([\n",
    "    Flux.flatten,\n",
    "    Dense(784, 200, relu),\n",
    "    Dense(200, 10)\n",
    "])\n",
    "image_seeds = [MNIST(:train)[i].features for i in 1:1]\n",
    "search_method = BFS(max_iter=1, batch_size=1)\n",
    "split_method = Bisect(1)\n",
    "output_set = BallInf(zeros(10), 1.0)\n",
    "onnx_model_path = \"/home/verification/ModelVerification.jl/debug.onnx\"\n",
    "Flux_model = model\n",
    "image_shape = (28, 28, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain([\n",
    "    Conv((3, 3), 3 => 8, relu, pad=SamePad(), stride=(2, 2)), #pad=SamePad() ensures size(output,d) == size(x,d) / stride.\n",
    "    BatchNorm(8),\n",
    "    MeanPool((2,2)),\n",
    "    SkipConnection(\n",
    "        Chain([\n",
    "            Conv((5, 5), 8 => 8, relu, pad=SamePad(), stride=(1, 1))\n",
    "            ]),\n",
    "        +\n",
    "    ),\n",
    "    Conv((3, 3), 8 => 8, relu, pad=SamePad(), stride=(2, 2)),\n",
    "    #ConvTranspose((3, 3), 8 => 4, relu, pad=SamePad(), stride=(2, 2)),#pad=SamePad() ensures size(output,d) == size(x,d) * stride.\n",
    "    Flux.flatten,\n",
    "    Dense(128, 100, relu),\n",
    "    Dense(100, 10)\n",
    "])\n",
    "testmode!(model)\n",
    "# image_seeds = CIFAR10(:train)[1:5].features # 32 x 32 x 3 x 5\n",
    "image_seeds = [CIFAR10(:train)[i].features for i in 1:2]\n",
    "# println(typeof(image_seeds[1][1,1,1,1]))\n",
    "search_method = BFS(max_iter=1, batch_size=1)\n",
    "split_method = Bisect(1)\n",
    "output_set = BallInf(zeros(10), 1.0)\n",
    "onnx_model_path = \"/home/verification/ModelVerification.jl/mlp.onnx\"\n",
    "Flux_model = model\n",
    "image_shape = (32, 32, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStar()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(onnx_model_path, Flux_model, image_shape, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStarZono()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(onnx_model_path, Flux_model, image_shape, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain([\n",
    "    Conv((3, 3), 3 => 128, relu, pad=SamePad(), stride=(2, 2)), #pad=SamePad() ensures size(output,d) == size(x,d) / stride.\n",
    "    BatchNorm(128),\n",
    "    MeanPool((2,2)),\n",
    "    SkipConnection(\n",
    "        Chain([\n",
    "            Conv((5, 5), 128 => 128, relu, pad=SamePad(), stride=(1, 1))\n",
    "            ]),\n",
    "        +\n",
    "    ),\n",
    "    ConvTranspose((3, 3), 128 => 128, relu, pad=SamePad(), stride=(2, 2)),#pad=SamePad() ensures size(output,d) == size(x,d) * stride.\n",
    "    Flux.flatten,\n",
    "    Dense(32768, 100, relu),\n",
    "    Dense(100, 10)\n",
    "])\n",
    "testmode!(model)\n",
    "# image_seeds = CIFAR10(:train)[1:5].features # 32 x 32 x 3 x 5\n",
    "image_seeds = [CIFAR10(:train)[i].features for i in 1:2]\n",
    "# println(typeof(image_seeds[1][1,1,1,1]))\n",
    "search_method = BFS(max_iter=1, batch_size=1)\n",
    "split_method = Bisect(1)\n",
    "output_set = BallInf(zeros(10), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStarZono()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(model, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profile.clear()\n",
    "@profile verify(search_method, split_method, prop_method, Problem(model, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"./prof.txt\", \"w\") do s\n",
    "    Profile.print(IOContext(s, :displaysize => (24, 500)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
