{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition relu_upper_bound(Any, Any) in module ModelVerification at /home/verification/ModelVerification.jl/src/propagate/operators/relu.jl:220 overwritten at /home/verification/ModelVerification.jl/src/branching/split.jl:63.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using ModelVerification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LazySets\n",
    "using PyCall\n",
    "using CSV\n",
    "using ONNX\n",
    "using Flux\n",
    "using Test\n",
    "using NNlib\n",
    "using ONNXNaiveNASflux\n",
    "using NaiveNASflux\n",
    "using Zygote\n",
    "# using DataFrames\n",
    "# import Flux: flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Flux: onehotbatch, onecold, flatten\n",
    "# using Flux.Losses: logitcrossentropy\n",
    "# using Statistics: mean\n",
    "using CUDA\n",
    "using MLDatasets: CIFAR10, MNIST\n",
    "using MLUtils: splitobs, DataLoader\n",
    "using Accessors\n",
    "using Profile\n",
    "using LinearAlgebra\n",
    "using Einsum\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params([Float32"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.622985;;], Float32"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.377015]])\n",
      "Dense"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{typeof(identity), CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}\n"
     ]
    }
   ],
   "source": [
    "x = cu([1])\n",
    "y = cu([2])\n",
    "model = Dense(1, 1) \n",
    "model = fmap(cu, model)\n",
    "loss(x, y) = Flux.mse(x, y)\n",
    "optimizer = Flux.Optimiser(Flux.ADAM(0.1))\n",
    "opt_state = Flux.setup(optimizer, model)\n",
    "for i in 1 : 500\n",
    "    grads = Flux.gradient(model) do m\n",
    "        result = m(x) \n",
    "        loss(result, y)\n",
    "    end\n",
    "    Flux.update!(opt_state, model, grads[1])\n",
    "end\n",
    "println(Flux.params(model))\n",
    "println(typeof(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "x = rand(2, 1, 5)\n",
    "weight = rand(1, 2)\n",
    "ans = NNlib.batched_mul(x, weight)\n",
    "println(size(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params(["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]])\n",
      "Params([[20], [1]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.0], [0.0]]\n"
     ]
    }
   ],
   "source": [
    "struct AlphaLayer\n",
    "    node\n",
    "    alpha\n",
    "    lower\n",
    "    unstable_mask\n",
    "    lower_mask \n",
    "    upper_slope\n",
    "    lower_bias\n",
    "    upper_bias\n",
    "end\n",
    "Flux.@functor AlphaLayer (alpha,)\n",
    "\n",
    "function (f::AlphaLayer)(x)\n",
    "    Last_A = x[1]\n",
    "    Last_bias = x[2]\n",
    "    lower_slope = clamp.(f.alpha, 0, 1) .* f.unstable_mask .+ f.lower_mask \n",
    "    if f.lower \n",
    "        New_A = bound_oneside(Last_A, lower_slope, f.upper_slope)\n",
    "    else\n",
    "        New_A = bound_oneside(Last_A, f.upper_slope, lower_slope)\n",
    "    end\n",
    "\n",
    "    if isnothing(Last_bias)\n",
    "        return [New_A, nothing]\n",
    "    end\n",
    "    New_bias = multiply_bias(Last_bias, f.upper_slope, f.upper_bias, f.lower_bias)\n",
    "\n",
    "    return [New_A, New_bias]\n",
    "end\n",
    "\n",
    "#Upper bound slope and intercept according to CROWN relaxation.\n",
    "function relu_upper_bound(lower, upper)\n",
    "    lower_r = clamp.(lower, -Inf, 0)\n",
    "    upper_r = clamp.(upper, 0, Inf)\n",
    "    upper_r .= max.(upper_r, lower_r .+ 1e-8)\n",
    "    upper_slope = upper_r ./ (upper_r .- lower_r) #the slope of the relu upper bound\n",
    "    upper_bias = - lower_r .* upper_slope #the bias of the relu upper bound\n",
    "    return upper_slope, upper_bias\n",
    "end\n",
    "\n",
    "function clamp_mutiply_A(last_A, slope_pos, slope_neg) \n",
    "    A_pos = clamp.(last_A, 0, Inf)\n",
    "    A_neg = clamp.(last_A, -Inf, 0)\n",
    "    slope_pos = repeat(reshape(slope_pos,(1, size(slope_pos)...)), size(A_pos)[1], 1, 1) #add spec dim for slope_pos\n",
    "    slope_neg = repeat(reshape(slope_neg,(1, size(slope_neg)...)), size(A_neg)[1], 1, 1) #add spec dim for slope_pos\n",
    "    New_A = slope_pos .* A_pos .+ slope_neg .* A_neg \n",
    "    return New_A\n",
    "end \n",
    "\n",
    "\n",
    "function clamp_mutiply_bias(last_A, bias_pos, bias_neg) \n",
    "    A_pos = clamp.(last_A, 0, Inf)\n",
    "    A_neg = clamp.(last_A, -Inf, 0) \n",
    "    if bias_pos !== nothing #new_bias_pos = torch.einsum('s...b,s...b->sb', A_pos, bias_pos)\n",
    "        new_bias_pos = zeros((size(A_pos)[1], size(A_pos)[end]))#spec_dim x batch dim\n",
    "        @einsum new_bias_pos[s,b] = A_pos[s,r,b] * bias_pos[r,b]\n",
    "    end\n",
    "\n",
    "    if bias_neg !== nothing #new_bias_neg = torch.einsum('...sb,...sb->sb', A_neg, bias_neg)\n",
    "        new_bias_neg = zeros((size(A_neg)[1], size(A_neg)[end]))#spec_dim x batch dim\n",
    "        @einsum new_bias_neg[s,b] = A_neg[s,r,b] * bias_neg[r,b]\n",
    "    end\n",
    "    New_bias = new_bias_pos .+ new_bias_neg\n",
    "    return New_bias\n",
    "end \n",
    "\n",
    "#using last_A for getting New_A\n",
    "function multiply_by_A_signs(last_A, slope_pos, slope_neg)\n",
    "    if ndims(slope_pos) == 1\n",
    "        # Special case for LSTM, the bias term is 1-dimension. \n",
    "        New_A = clamp.(last_A, 0, Inf) .* slope_pos .+ clamp.(last_A, -Inf, 0) .* slope_neg\n",
    "    else\n",
    "        New_A = clamp_mutiply_A(last_A, slope_pos, slope_neg)\n",
    "        return New_A\n",
    "    end\n",
    "end\n",
    "\n",
    "function multiply_bias(last_A, upper_slope, bias_pos, bias_neg)\n",
    "    if ndims(upper_slope) == 1\n",
    "        # Special case for LSTM, the bias term is 1-dimension. \n",
    "        New_bias = clamp.(last_A, 0, Inf) .* bias_pos .+ clamp.(last_A, -Inf, 0) .* bias_neg\n",
    "    else\n",
    "        New_bias = clamp_mutiply_bias(last_A, bias_pos, bias_neg)\n",
    "        return New_bias\n",
    "    end\n",
    "end\n",
    "\n",
    "#bound oneside of the relu, like upper or lower\n",
    "function bound_oneside(last_A, slope_pos, slope_neg)\n",
    "    if isnothing(last_A)\n",
    "        return nothing, nothing\n",
    "    end\n",
    "    New_A = multiply_by_A_signs(last_A, slope_pos, slope_neg)\n",
    "    return New_A\n",
    "end\n",
    "\n",
    "\n",
    "alpha_lower = [20]\n",
    "alpha_upper = [1]\n",
    "unstable_mask = [1]\n",
    "lower_mask = [1]\n",
    "upper_slope = [2]\n",
    "upper_bias = [0]\n",
    "lower_bias = [0]\n",
    "lower = upper = true\n",
    "if lower == true\n",
    "    Alpha_Lower_Layer = AlphaLayer(\"relu_1\", alpha_lower, true, unstable_mask, lower_mask, upper_slope, upper_bias, lower_bias)\n",
    "end\n",
    "if upper ==true\n",
    "    Alpha_Upper_Layer = AlphaLayer(\"relu_1\", alpha_upper, false, unstable_mask, lower_mask, upper_slope, lower_bias, upper_bias)\n",
    "end\n",
    "a = []\n",
    "push!(a, Alpha_Lower_Layer) \n",
    "push!(a, Alpha_Upper_Layer)\n",
    "println(Flux.params(Alpha_Lower_Layer))\n",
    "println(Flux.params(a)) \n",
    "a = Chain(a)\n",
    "println(a([2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activation_node in model_info.activation_nodes\n",
    "    batch_info[activation_node][:split_active] = []\n",
    "end\n",
    "primals, duals, mini_inp = None, None, None\n",
    "upper_bound = zeros(size(lower_bound)) .+ Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(value = \u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m, time = 241.623956484, bytes = 12053469184, gctime = 5.139455547, gcstats = Base.GC_Diff(12053469184, 218, 0, 220486006, 90512, 2341, 5139455547, 36, 0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function test_mlp(prop_method)\n",
    "    small_nnet_file = \"/home/verification/ModelVerification.jl/test/networks/small_nnet.nnet\"\n",
    "    # small_nnet encodes the simple function 24*max(x + 1.5, 0) + 18.5\n",
    "    small_nnet = read_nnet(small_nnet_file, last_layer_activation = ModelVerification.ReLU())\n",
    "    flux_model = Flux.Chain(small_nnet)\n",
    "    #ONNXNaiveNASflux.save(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", flux_model, (1,1))\n",
    "    #println(flux_model)\n",
    "    #println(flux_model.layers[1].weight, \" \", flux_model.layers[1].bias) # max(x+1.5, 0) max(x+1.5, 0)              [0,4]\n",
    "    #println(flux_model.layers[2].weight, \" \", flux_model.layers[2].bias) # 4*max(x+1.5, 0)+2.5 4*max(x+1.5, 0)+2.5  [2.5, 18.5]\n",
    "    #println(flux_model.layers[3].weight, \" \", flux_model.layers[3].bias) # 24*max(x+1.5, 0)+18.5                    [18.5, 114.5]\n",
    "    in_hyper  = Hyperrectangle(low = [-2.5], high = [2.5]) # expected out: [18.5, 114.5]\n",
    "    out_violated    = Hyperrectangle(low = [19], high = [114]) # 20.0 ≤ y ≤ 90.0\n",
    "    out_holds = Hyperrectangle(low = [18], high = [115.0]) # -1.0 ≤ y ≤ 50.0\n",
    "    comp_violated    = Complement(Hyperrectangle(low = [10], high = [19])) # y ≤ 10.0 or 19 ≤ y\n",
    "    comp_holds    = Complement(Hyperrectangle(low = [115], high = [118])) # y ≤ 10.0 or 18 ≤ y\n",
    "    info = nothing\n",
    "    search_method = BFS(max_iter=100, batch_size=1)\n",
    "    split_method = Bisect(1)\n",
    "    @test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, out_holds)).status == :holds\n",
    "    #@test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, out_violated)).status == :violated\n",
    "    #@test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, comp_holds)).status == :holds\n",
    "    #@test verify(search_method, split_method, prop_method, Problem(\"/home/verification/ModelVerification.jl/small_nnet.onnx\", in_hyper, comp_violated)).status == :violated\n",
    "    #= @test verify(search_method, split_method, prop_method, Problem(flux_model, in_hyper, out_holds)).status == :holds\n",
    "    @test verify(search_method, split_method, prop_method, Problem(flux_model, in_hyper, out_violated)).status == :violated\n",
    "    @test verify(search_method, split_method, prop_method, Problem(flux_model, in_hyper, comp_holds)).status == :holds\n",
    "    @test verify(search_method, split_method, prop_method, Problem(flux_model, in_hyper, comp_violated)).status == :violated =#\n",
    "end\n",
    "@timed begin\n",
    "    #for i in 1:1\n",
    "        test_mlp(AlphaCrown(Crown(true, true), true, false, Flux.Optimiser(Flux.ADAM(0.1)), 10))\n",
    "        #test_mlp(Ai2z())\n",
    "        #test_mlp(Crown(true, true))\n",
    "        #test_mlp(StarSet(Crown(true, true)))\n",
    "    #end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 12, 3, 24, 35]\n"
     ]
    }
   ],
   "source": [
    "vector = [1, 2, 3, 4, 5]\n",
    "values = [10, 20, 30]\n",
    "indices = [2, 4, 5]\n",
    "\n",
    "vector[indices] += values \n",
    "\n",
    "println(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08754564 0.2532909 0.0570138 0.0309838 0.09425839999999999 0.24900168 0.21692964 0.0656158 0.64241074 0.05683992;;; 0.02090088 0.25883952000000005 0.07066605000000001 0.1266686 0.023623599999999998 0.18000204 0.23194958000000002 0.0661486 0.22239152 0.05627592]\n",
      "(1, 10, 2)\n",
      "(10, 2)\n",
      "[0.0 0.0; 0.0 0.0; 0.0 0.0; 0.0 0.0; 0.094022754 0.0; 0.0 0.0; 0.0 0.007538361350000008; 0.02440907760000001 0.0; 0.0 0.12453925120000002; 0.02299174764 0.0]\n"
     ]
    }
   ],
   "source": [
    "function relu_upper_bound(lower, upper)\n",
    "    lower_r = clamp.(lower, -Inf, 0)\n",
    "    upper_r = clamp.(upper, 0, Inf)\n",
    "    upper_r .= max.(upper_r, lower_r .+ 1e-8)\n",
    "    upper_slope = upper_r ./ (upper_r .- lower_r) #the slope of the relu upper bound\n",
    "    upper_bias = - lower_r .* upper_slope #the bias of the relu upper bound\n",
    "    return upper_slope, upper_bias\n",
    "end\n",
    "\n",
    "lower = [0.0733 -0.2387; -0.4172 0.4561; 0.4147 -0.2835; 0.2980 -0.4698; -0.0005 0.2590; 0.2553 0.0367; -0.2080 -0.1935; -0.1256 0.3723; 0.4952 -0.0880; -0.1191 -0.2540]\n",
    "A = [0.8017 0.7030 0.6828 0.1961 0.3994 0.3634 0.7698 0.8867 0.9313 0.5039;;; 0.1914 0.7184 0.8463 0.8017 0.1001 0.2627 0.8231 0.8939 0.3224 0.4989]\n",
    "upper = lower .+ 0.2\n",
    "unstable_mask = (upper .> 0) .& (lower .< 0)\n",
    "unstable_mask = reshape(unstable_mask, (1, size(unstable_mask)...))\n",
    "upper_slope, upper_bias = relu_upper_bound(lower, upper)\n",
    "intercept_temp = clamp.(A, -Inf, 0)\n",
    "intercept_candidate = intercept_temp .* reshape(upper_bias, (1, size(upper_bias)...))\n",
    "b_temp = [0.1092, 0.3603, 0.0835, 0.1580, 0.2360, 0.6852, 0.2818, 0.0740, 0.6898, 0.1128]\n",
    "b_temp = reshape(b_temp, (1, size(b_temp)...)) .* A\n",
    "println(b_temp)\n",
    "println(size(b_temp))\n",
    "upper_slope = reshape(upper_slope, (1, size(upper_slope)...))\n",
    "bias_candidate_1 = b_temp .* (upper_slope .- 1)\n",
    "bias_candidate_2 = b_temp .* upper_slope\n",
    "bias_candidate = max.(bias_candidate_1, bias_candidate_2)\n",
    "score_candidate = bias_candidate .+ intercept_candidate\n",
    "score_candidate = dropdims(mean((abs.(score_candidate) .* unstable_mask), dims = 1), dims = 1)\n",
    "println(size(score_candidate))\n",
    "println(score_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function relu_upper_bound(lower, upper)\n",
    "    lower_r = clamp.(lower, -Inf, 0)\n",
    "    upper_r = clamp.(upper, 0, Inf)\n",
    "    upper_r .= max.(upper_r, lower_r .+ 1e-8)\n",
    "    upper_slope = upper_r ./ (upper_r .- lower_r) #the slope of the relu upper bound\n",
    "    upper_bias = - lower_r .* upper_slope #the bias of the relu upper bound\n",
    "    return upper_slope, upper_bias\n",
    "end\n",
    "\n",
    "function branching_scores_kfsb(A, model_info, batch_info)\n",
    "    score = []\n",
    "    for node in reverse(model_info.activation_nodes)\n",
    "        layer = model_info.node_layer[node]\n",
    "        unstable_mask = batch_info[node][:unstable_mask]\n",
    "        unstable_mask = reshape(unstable_mask, (1, size(unstable_mask)...))\n",
    "        lower = batch_info[node][:pre_lower]\n",
    "        upper = batch_info[node][:pre_upper]\n",
    "        upper_slope, upper_bias = relu_upper_bound(lower, upper)\n",
    "\n",
    "        intercept_temp = clamp.(A, -Inf, 0)\n",
    "        intercept_candidate = intercept_temp .* reshape(upper_bias, (1, size(upper_bias)...))\n",
    "\n",
    "        input_node = model_info.pre_layer[node][1]\n",
    "        input_layer = model_info.node_layer[input_node]\n",
    "        if isa(layer, Flux.Conv)\n",
    "            if !isnothing(input_layer.bias)\n",
    "                b_temp = input_layer.bias\n",
    "            else\n",
    "                b_temp = 0\n",
    "            end\n",
    "        elseif isa(layer, Flux.Dense)\n",
    "            if !isnothing(input_layer.bias)\n",
    "                b_temp = input_layer.bias\n",
    "            else\n",
    "                b_temp = 0\n",
    "            end\n",
    "        elseif isa(layer, +)\n",
    "            b_temp = 0\n",
    "            for l in model_info.pre_layer[input_node]\n",
    "                l_layer = model_info.node_layer[l]\n",
    "                if isa(layer, Flux.Conv)\n",
    "                    if length(l_layer.inputs) > 2\n",
    "                        b_temp += typeof.bias\n",
    "                    end\n",
    "                end\n",
    "                if isa(layer, Flux.normalise)\n",
    "                    b_temp += 0\n",
    "                end\n",
    "                if isa(layer, +)\n",
    "                    for ll in model_info.pre_layer[l]\n",
    "                        ll_layer = model_info.node_layer[ll]\n",
    "                        if isa(layer, Flux.Conv)\n",
    "                            b_temp += ll_layer.bias\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        else\n",
    "            b_temp = 0\n",
    "        end\n",
    "        \n",
    "        b_temp = reshape(b_temp, (1, size(b_temp)...)) .* A\n",
    "        bias_candidate_1 = b_temp .* (upper_slope .- 1)\n",
    "        bias_candidate_2 = b_temp .* upper_slope\n",
    "        bias_candidate = max.(bias_candidate_1, bias_candidate_2)\n",
    "        score_candidate = bias_candidate .+ intercept_candidate\n",
    "        score_candidate = dropdims(mean((abs.(score_candidate) .* unstable_mask), dims = 1), dims = 1)\n",
    "        push!(score, score_candidate)\n",
    "    end\n",
    "    return score\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "(2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "a = [1.0; -1.0;;;]\n",
    "println(size(a))\n",
    "b = [18.5;;]\n",
    "println(size(b))\n",
    "c = batched_vec(a, b)\n",
    "println(size(c))\n",
    "d = batched_mul(a, b)\n",
    "println(size(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1)\n",
      "[18.5;;;]\n"
     ]
    }
   ],
   "source": [
    "last_A = [1;;;]\n",
    "println(size(last_A))\n",
    "x = [18.5;;]\n",
    "println(size(x))\n",
    "bias = [0;;]\n",
    "println(size(bias))\n",
    "out = NNlib.batched_mul(last_A, x) .+ bias\n",
    "println(size(out))\n",
    "println(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5; 1.5;;]\n",
      "(1, 1, 1)\n",
      "[28.799999999999997;;;]\n"
     ]
    }
   ],
   "source": [
    "last_A = [9.6 9.6;;;]\n",
    "bias = [1.5; 1.5;;]\n",
    "println(bias)\n",
    "New_bias = NNlib.batched_mul(last_A, bias)\n",
    "println(size(New_bias))\n",
    "println(New_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain([\n",
    "    Conv((3, 3), 3 => 8, relu, pad=SamePad(), stride=(2, 2)), #pad=SamePad() ensures size(output,d) == size(x,d) / stride.\n",
    "    BatchNorm(8),\n",
    "    MeanPool((2,2)),\n",
    "    SkipConnection(\n",
    "        Chain([\n",
    "            Conv((5, 5), 8 => 8, relu, pad=SamePad(), stride=(1, 1))\n",
    "            ]),\n",
    "        +\n",
    "    ),\n",
    "    #ConvTranspose((3, 3), 8 => 4, relu, pad=SamePad(), stride=(2, 2)),#pad=SamePad() ensures size(output,d) == size(x,d) * stride.\n",
    "    Flux.flatten,\n",
    "    Dense(512, 100, relu),\n",
    "    Dense(100, 10)\n",
    "])\n",
    "testmode!(model)\n",
    "# image_seeds = CIFAR10(:train)[1:5].features # 32 x 32 x 3 x 5\n",
    "image_seeds = [CIFAR10(:train)[i].features for i in 1:2]\n",
    "input_set = ImageConvexHull(image_seeds)\n",
    "# println(typeof(image_seeds[1][1,1,1,1]))\n",
    "search_method = BFS(max_iter=1, batch_size=1)\n",
    "split_method = Bisect(1)\n",
    "output_set = BallInf(zeros(10), 1.0)\n",
    "onnx_model_path = \"/home/verification/ModelVerification.jl/mlp.onnx\"\n",
    "flux_model = model\n",
    "image_shape = (32, 32, 3, 5)\n",
    "println(image_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStar()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(flux_model, input_set, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStarZono()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(onnx_model_path, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain([\n",
    "    Flux.flatten,\n",
    "    Dense(784, 200, relu),\n",
    "    Dense(200, 10)\n",
    "])\n",
    "image_seeds = [MNIST(:train)[i].features for i in 1:1]\n",
    "search_method = BFS(max_iter=1, batch_size=1)\n",
    "split_method = Bisect(1)\n",
    "output_set = BallInf(zeros(10), 1.0)\n",
    "onnx_model_path = \"/home/verification/ModelVerification.jl/debug.onnx\"\n",
    "Flux_model = model\n",
    "image_shape = (28, 28, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain([\n",
    "    Conv((3, 3), 3 => 8, relu, pad=SamePad(), stride=(2, 2)), #pad=SamePad() ensures size(output,d) == size(x,d) / stride.\n",
    "    BatchNorm(8),\n",
    "    MeanPool((2,2)),\n",
    "    SkipConnection(\n",
    "        Chain([\n",
    "            Conv((5, 5), 8 => 8, relu, pad=SamePad(), stride=(1, 1))\n",
    "            ]),\n",
    "        +\n",
    "    ),\n",
    "    Conv((3, 3), 8 => 8, relu, pad=SamePad(), stride=(2, 2)),\n",
    "    #ConvTranspose((3, 3), 8 => 4, relu, pad=SamePad(), stride=(2, 2)),#pad=SamePad() ensures size(output,d) == size(x,d) * stride.\n",
    "    Flux.flatten,\n",
    "    Dense(128, 100, relu),\n",
    "    Dense(100, 10)\n",
    "])\n",
    "testmode!(model)\n",
    "# image_seeds = CIFAR10(:train)[1:5].features # 32 x 32 x 3 x 5\n",
    "image_seeds = [CIFAR10(:train)[i].features for i in 1:2]\n",
    "# println(typeof(image_seeds[1][1,1,1,1]))\n",
    "search_method = BFS(max_iter=1, batch_size=1)\n",
    "split_method = Bisect(1)\n",
    "output_set = BallInf(zeros(10), 1.0)\n",
    "onnx_model_path = \"/home/verification/ModelVerification.jl/mlp.onnx\"\n",
    "Flux_model = model\n",
    "image_shape = (32, 32, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStar()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(onnx_model_path, Flux_model, image_shape, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStarZono()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(onnx_model_path, Flux_model, image_shape, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain([\n",
    "    Conv((3, 3), 3 => 128, relu, pad=SamePad(), stride=(2, 2)), #pad=SamePad() ensures size(output,d) == size(x,d) / stride.\n",
    "    BatchNorm(128),\n",
    "    MeanPool((2,2)),\n",
    "    SkipConnection(\n",
    "        Chain([\n",
    "            Conv((5, 5), 128 => 128, relu, pad=SamePad(), stride=(1, 1))\n",
    "            ]),\n",
    "        +\n",
    "    ),\n",
    "    ConvTranspose((3, 3), 128 => 128, relu, pad=SamePad(), stride=(2, 2)),#pad=SamePad() ensures size(output,d) == size(x,d) * stride.\n",
    "    Flux.flatten,\n",
    "    Dense(32768, 100, relu),\n",
    "    Dense(100, 10)\n",
    "])\n",
    "testmode!(model)\n",
    "# image_seeds = CIFAR10(:train)[1:5].features # 32 x 32 x 3 x 5\n",
    "image_seeds = [CIFAR10(:train)[i].features for i in 1:2]\n",
    "# println(typeof(image_seeds[1][1,1,1,1]))\n",
    "search_method = BFS(max_iter=1, batch_size=1)\n",
    "split_method = Bisect(1)\n",
    "output_set = BallInf(zeros(10), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_method = ImageStarZono()\n",
    "@timed verify(search_method, split_method, prop_method, Problem(model, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profile.clear()\n",
    "@profile verify(search_method, split_method, prop_method, Problem(model, image_seeds, output_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"./prof.txt\", \"w\") do s\n",
    "    Profile.print(IOContext(s, :displaysize => (24, 500)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
