{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLUtils: splitobs, unsqueeze\n",
    "using Flux\n",
    "using Flux: onehotbatch\n",
    "using Flux.Data: DataLoader\n",
    "using MLDatasets, Flux, JLD2, CUDA  # this will install everything if necc.\n",
    "\n",
    "using Statistics: mean  # standard library\n",
    "using ImageCore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder = \"runs\"  # sub-directory in which to save\n",
    "isdir(folder) || mkdir(folder)\n",
    "filename = joinpath(folder, \"lenet.jld2\")\n",
    "@show filename\n",
    "train_data = MLDatasets.MNIST()  # i.e. split=:train\n",
    "test_data = MLDatasets.MNIST(split=:test)\n",
    "\n",
    "function loader(data::MNIST=train_data; batchsize::Int=64)\n",
    "    x4dim = reshape(data.features, 28,28,1,:)   # insert trivial channel dim\n",
    "    yhot = Flux.onehotbatch(data.targets, 0:9)  # make a 10×60000 OneHotMatrix\n",
    "    Flux.DataLoader((x4dim, yhot); batchsize, shuffle=true) \n",
    "end\n",
    "\n",
    "x1, y1 = first(loader()); # (28×28×1×64 Array{Float32, 3}, 10×64 OneHotMatrix(::Vector{UInt32}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×64 Matrix{Float32}:\n",
       " -0.0039766   0.00916768  -0.0995651   -0.0273454  …   0.0706634  -0.0748312\n",
       " -0.0816891  -0.0597531   -0.08209     -0.0160386     -0.155549   -0.0893364\n",
       "  0.0570726   0.0269321    0.0852055   -0.0553192      0.0806991   0.073685\n",
       " -0.021709   -0.0199056   -0.0163364   -0.0893441     -0.0567437   0.00870506\n",
       " -0.183569   -0.0962708   -0.170495    -0.151974      -0.161699   -0.168436\n",
       " -0.0349672   0.0627827   -0.00354918  -0.0508724  …  -0.0322047   0.0960027\n",
       " -0.154052   -0.0875013   -0.16829     -0.187637      -0.103768   -0.142866\n",
       " -0.0769983  -0.0138387   -0.0464541   -0.0603695     -0.0597908  -0.0429375\n",
       "  0.0214283  -0.0236385   -0.0170059    0.024636       0.0277364  -0.0282913\n",
       "  0.0575947   0.0729606    0.0815842    0.0470092      0.0874391   0.0230748"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lenet = Chain(\n",
    "    Conv((5, 5), 1=>6, relu),\n",
    "    MaxPool((2, 2)),\n",
    "    Conv((5, 5), 6=>16, relu),\n",
    "    MaxPool((2, 2)),\n",
    "    Flux.flatten,\n",
    "    Dense(256 => 120, relu),\n",
    "    Dense(120 => 84, relu), \n",
    "    Dense(84 => 10),\n",
    ")\n",
    "# ) |> gpu\n",
    "\n",
    "# sum(softmax(y1hat); dims=1)\n",
    "\n",
    "# @show y1hat\n",
    "# @show hcat(Flux.onecold(y1hat, 0:9), Flux.onecold(y1, 0:9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function loss_and_accuracy(model, data::MNIST=test_data)\n",
    "    (x,y) = only(loader(data; batchsize=length(data)))  # make one big batch\n",
    "    ŷ = model(x)\n",
    "    loss = Flux.logitcrossentropy(ŷ, y)  # did not include softmax in the model\n",
    "    acc = round(100 * mean(Flux.onecold(ŷ) .== Flux.onecold(y)); digits=2)\n",
    "    (; loss, acc, split=data.split)  # return a NamedTuple\n",
    "end\n",
    "\n",
    "@show loss_and_accuracy(lenet);  # accuracy about 10%, before training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#===== TRAINING =====#\n",
    "settings = (;\n",
    "    eta = 3e-4,     # learning rate\n",
    "    lambda = 1e-2,  # for weight decay\n",
    "    batchsize = 128,\n",
    "    epochs = 10,\n",
    ")\n",
    "train_log = []\n",
    "\n",
    "opt_rule = OptimiserChain(WeightDecay(settings.lambda), Adam(settings.eta))\n",
    "opt_state = Flux.setup(opt_rule, lenet);\n",
    "\n",
    "for epoch in 1:settings.epochs\n",
    "    # @time will show a much longer time for the first epoch, due to compilation\n",
    "    @time for (x,y) in loader(batchsize=settings.batchsize)\n",
    "        grads = Flux.gradient(m -> Flux.logitcrossentropy(m(x), y), lenet)\n",
    "        Flux.update!(opt_state, lenet, grads[1])\n",
    "    end\n",
    "    # Logging & saving, but not on every epoch\n",
    "    if epoch % 2 == 1\n",
    "        loss, acc, _ = loss_and_accuracy(lenet)\n",
    "        test_loss, test_acc, _ = loss_and_accuracy(lenet, test_data)\n",
    "        @info \"logging:\" epoch acc test_acc\n",
    "        nt = (; epoch, loss, acc, test_loss, test_acc)  # make a NamedTuple\n",
    "        push!(train_log, nt)\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Any}:\n",
       " (epoch = 1, loss = 0.21469705f0, acc = 93.69, test_loss = 0.21469706f0, test_acc = 93.69)\n",
       " (epoch = 3, loss = 0.12560661f0, acc = 96.26, test_loss = 0.12560661f0, test_acc = 96.26)\n",
       " (epoch = 5, loss = 0.105488405f0, acc = 97.11, test_loss = 0.105488405f0, test_acc = 97.11)\n",
       " (epoch = 7, loss = 0.09284543f0, acc = 97.37, test_loss = 0.09284544f0, test_acc = 97.37)\n",
       " (epoch = 9, loss = 0.08910663f0, acc = 97.32, test_loss = 0.08910663f0, test_acc = 97.32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JLD2.jldsave(filename; lenet_state = Flux.state(lenet) |> cpu)\n",
    "println(\"saved to \", filename, \" after \", epoch, \" epochs\")\n",
    "\n",
    "# We can re-run the quick sanity-check of predictions:\n",
    "y1hat = lenet(x1)\n",
    "@show hcat(Flux.onecold(y1hat, 0:9), Flux.onecold(y1, 0:9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#===== INSPECTION =====#\n",
    "\n",
    "\n",
    "\n",
    "xtest, ytest = only(loader(test_data, batchsize=length(test_data)));\n",
    "\n",
    "# There are many ways to look at images, you won't need ImageInTerminal if working in a notebook.\n",
    "# ImageCore.Gray is a special type, whick interprets numbers between 0.0 and 1.0 as shades:\n",
    "\n",
    "xtest[:,:,1,5] .|> Gray |> transpose |> cpu\n",
    "\n",
    "\n",
    "loaded_state = JLD2.load(filename, \"lenet_state\");\n",
    "Flux.loadmodel!(lenet2, loaded_state)\n",
    "\n",
    "@show lenet2(cpu(x1)) ≈ cpu(lenet(x1));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
