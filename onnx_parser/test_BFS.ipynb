{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Images, ImageIO\n",
    "using ONNXNaiveNASflux, NaiveNASflux, .NaiveNASlib\n",
    "using LinearAlgebra\n",
    "using OpenCV\n",
    "using Flux\n",
    "using CSV\n",
    "using DataFrames\n",
    "using DataStructures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_flux_model (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "function square_img(image, bbox, img_size)\n",
    "    bbox_width = round(Int, bbox[\"bbox_x2\"] - bbox[\"bbox_x1\"])\n",
    "    bbox_height = round(Int, bbox[\"bbox_y2\"] - bbox[\"bbox_y1\"])\n",
    "    offset_tl = Int.([bbox[\"bbox_x1\"], bbox[\"bbox_y1\"]] .+ 1)\n",
    "    h, _ = img_size\n",
    "    if bbox_width >= bbox_height\n",
    "        sq_img = zeros(UInt8, bbox_width, bbox_width, 3)\n",
    "        if bbox_width >= h\n",
    "            crop_img = image[:, offset_tl[1]: offset_tl[1] + bbox_width - 1, :]\n",
    "            offset_tl[2] = 1\n",
    "            bbox_height = h\n",
    "        elseif offset_tl[2] + bbox_width - 1 >= h\n",
    "            offset_tl[2] = h + 1 - bbox_width\n",
    "            bbox_height = bbox_width\n",
    "            crop_img = image[offset_tl[2] : offset_tl[2] + bbox_width - 1, offset_tl[1] : offset_tl[1] + bbox_width - 1, :]\n",
    "        else\n",
    "            crop_img = image[offset_tl[2] : offset_tl[2] + bbox_width - 1, offset_tl[1]+1 : offset_tl[1] + bbox_width - 1, :]\n",
    "            bbox_height = bbox_width\n",
    "        end\n",
    "        sq_img[1:bbox_height, 1:bbox_width, :] = crop_img\n",
    "    else\n",
    "        sq_img = zeros(UInt8, bbox_height, bbox_height, 3)\n",
    "    end\n",
    "    return sq_img, offset_tl\n",
    "end\n",
    "\n",
    "function preprocess_image(img_path, csv_path)\n",
    "    image = OpenCV.imread(img_path)\n",
    "    _, w, h = size(image)  # (3, 1920, 1200)\n",
    "    # CWH, BGR channel for Julia OpenCV, we desire WHCN for Flux model\n",
    "    image = OpenCV.cvtColor(image, OpenCV.COLOR_BGR2GRAY)\n",
    "    image = OpenCV.cvtColor(image, OpenCV.COLOR_GRAY2RGB)\n",
    "    image = permutedims(image, (3, 2, 1))\n",
    "    \n",
    "    truth_df = DataFrame(CSV.File(csv_path))\n",
    "    bbox = truth_df[!, [:bbox_x1, :bbox_y1, :bbox_x2, :bbox_y2]]\n",
    "    bbox = bbox[parse(Int, split(img_path[1:end-4], '_')[end]) + 1, :]\n",
    "    sq_image, offset_tl = square_img(image, bbox, (h, w))\n",
    "    input_size = 256\n",
    "    sq_image = permutedims(sq_image, (3, 2, 1))  # no need to permute multiple times, will remove it later\n",
    "    resize_img = OpenCV.resize(sq_image, OpenCV.Size{Int32}(input_size, input_size), interpolation=OpenCV.INTER_AREA)\n",
    "    resize_img = resize_img / 255.0\n",
    "    resize_img = permutedims(resize_img, (3, 2, 1))\n",
    "    return resize_img\n",
    "end\n",
    "\n",
    "function get_parallel_chains(comp_vertices, index_more_than_one_outputs)\n",
    "    function get_chain(vertex)\n",
    "        m = Any[]\n",
    "        curr_vertex = vertex\n",
    "        while length(inputs(curr_vertex)) == 1\n",
    "            # println(\"curr vertex \", name(curr_vertex))\n",
    "            push!(m, layer(curr_vertex))\n",
    "            curr_vertex = outputs(curr_vertex)[1]\n",
    "        end\n",
    "        return Chain(m...), curr_vertex\n",
    "    end\n",
    "    outs = outputs(comp_vertices[index_more_than_one_outputs])\n",
    "    @assert length(outs) == 2\n",
    "    chain1, vertex_more_than_one_inputs = get_chain(outs[1])\n",
    "    chain2, _ = get_chain(outs[2])\n",
    "    @assert occursin(\"Add\", name(vertex_more_than_one_inputs))\n",
    "    inner_iter = findfirst(v -> name(v) == name(vertex_more_than_one_inputs), comp_vertices)\n",
    "    if length(chain1) == 0\n",
    "        return SkipConnection(chain2, (+)), inner_iter\n",
    "    elseif length(chain2) == 0\n",
    "        return SkipConnection(chain1, (+)), inner_iter\n",
    "    else\n",
    "        return Parallel(+; α = chain1, β = chain2), inner_iter\n",
    "    end\n",
    "end\n",
    "\n",
    "function build_flux_model(onnx_model_path)\n",
    "    comp_graph = ONNXNaiveNASflux.load(onnx_model_path)\n",
    "    # find mean value\n",
    "    model_vec = Any[]\n",
    "    # sub_vertices = findvertices(\"/Sub\", comp_graph)\n",
    "    # if !isempty(sub_vertices)\n",
    "    #     img_mean = inputs(sub_vertices[1])[2]()\n",
    "    #     println(img_mean)\n",
    "    #     # println(inputs(vertices(comp_graph)[5])[1]())\n",
    "\n",
    "    #     push!(model_vec, x -> x .- img_mean)\n",
    "    # end\n",
    "    \n",
    "\n",
    "    inner_iter = 0\n",
    "    for (index, vertex) in enumerate(vertices(comp_graph))\n",
    "        if index < 5 || index <= inner_iter\n",
    "            continue\n",
    "        end \n",
    "        if string(layer(vertex)) == \"#213\"\n",
    "            push!(model_vec, NNlib.relu)\n",
    "        else\n",
    "            push!(model_vec, layer(vertex))\n",
    "        end\n",
    "        if length(outputs(vertex)) > 1\n",
    "            # println(\"name: \", name(vertex))\n",
    "            parallel_chain, inner_iter = get_parallel_chains(vertices(comp_graph), index)\n",
    "            push!(model_vec, parallel_chain)\n",
    "        end\n",
    "    end\n",
    "    model = Chain(model_vec...)\n",
    "    Flux.testmode!(model)\n",
    "    return (model)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set(Any[\"BatchNormalization\", \"Div\", \"Relu\", \"Constant\", \"Sub\", \"Conv\", \"ConvTranspose\", \"AveragePool\", \"Add\"])\n",
      "onnx::Sub_0\n",
      "NaiveNASflux.LayerTypeWrapper{NaiveNASflux.GenericFluxConvolutional{2}}(NaiveNASflux.GenericFluxConvolutional{2}())\n",
      "/Constant\n",
      "nothing\n",
      "/Sub\n",
      "#341\n",
      "/Div\n",
      "#202\n",
      "/conv1/Conv\n",
      "Conv((7, 7), 3 => 64, pad=3, stride=2, bias=false)\n",
      "/bn1/BatchNormalization\n",
      "BatchNorm(64, relu)\n",
      "/avgpool/AveragePool\n",
      "MeanPool((3, 3), pad=1, stride=2)\n",
      "/layer1/layer1.0/conv1/Conv\n",
      "Conv((3, 3), 64 => 64, pad=1, bias=false)\n",
      "/layer1/layer1.0/bn1/BatchNormalization\n",
      "BatchNorm(64, relu)\n",
      "/layer1/layer1.0/conv2/Conv\n",
      "Conv((3, 3), 64 => 64, pad=1, bias=false)\n",
      "/layer1/layer1.0/bn2/BatchNormalization\n",
      "BatchNorm(64)\n",
      "/layer1/layer1.0/Add\n",
      "#341\n",
      "/layer1/layer1.0/relu_1/Relu\n",
      "#213\n",
      "/layer1/layer1.1/conv1/Conv\n",
      "Conv((3, 3), 64 => 64, pad=1, bias=false)\n",
      "/layer1/layer1.1/bn1/BatchNormalization\n",
      "BatchNorm(64, relu)\n",
      "/layer1/layer1.1/conv2/Conv\n",
      "Conv((3, 3), 64 => 64, pad=1, bias=false)\n",
      "/layer1/layer1.1/bn2/BatchNormalization\n",
      "BatchNorm(64)\n",
      "/layer1/layer1.1/Add\n",
      "#341\n",
      "/layer1/layer1.1/relu_1/Relu\n",
      "#213\n",
      "/layer2/layer2.0/conv1/Conv\n",
      "Conv((3, 3), 64 => 128, pad=1, stride=2, bias=false)\n",
      "/layer2/layer2.0/bn1/BatchNormalization\n",
      "BatchNorm(128, relu)\n",
      "/layer2/layer2.0/conv2/Conv\n",
      "Conv((3, 3), 128 => 128, pad=1, bias=false)\n",
      "/layer2/layer2.0/bn2/BatchNormalization\n",
      "BatchNorm(128)\n",
      "/layer2/layer2.0/downsample/downsample.0/Conv\n",
      "Conv((1, 1), 64 => 128, stride=2, bias=false)\n",
      "/layer2/layer2.0/downsample/downsample.1/BatchNormalization\n",
      "BatchNorm(128)\n",
      "/layer2/layer2.0/Add\n",
      "#341\n",
      "/layer2/layer2.0/relu_1/Relu\n",
      "#213\n",
      "/layer2/layer2.1/conv1/Conv\n",
      "Conv((3, 3), 128 => 128, pad=1, bias=false)\n",
      "/layer2/layer2.1/bn1/BatchNormalization\n",
      "BatchNorm(128, relu)\n",
      "/layer2/layer2.1/conv2/Conv\n",
      "Conv((3, 3), 128 => 128, pad=1, bias=false)\n",
      "/layer2/layer2.1/bn2/BatchNormalization\n",
      "BatchNorm(128)\n",
      "/layer2/layer2.1/Add\n",
      "#341\n",
      "/layer2/layer2.1/relu_1/Relu\n",
      "#213\n",
      "/layer3/layer3.0/conv1/Conv\n",
      "Conv((3, 3), 128 => 256, pad=1, stride=2, bias=false)\n",
      "/layer3/layer3.0/bn1/BatchNormalization\n",
      "BatchNorm(256, relu)\n",
      "/layer3/layer3.0/conv2/Conv\n",
      "Conv((3, 3), 256 => 256, pad=1, bias=false)\n",
      "/layer3/layer3.0/bn2/BatchNormalization\n",
      "BatchNorm(256)\n",
      "/layer3/layer3.0/downsample/downsample.0/Conv\n",
      "Conv((1, 1), 128 => 256, stride=2, bias=false)\n",
      "/layer3/layer3.0/downsample/downsample.1/BatchNormalization\n",
      "BatchNorm(256)\n",
      "/layer3/layer3.0/Add\n",
      "#341\n",
      "/layer3/layer3.0/relu_1/Relu\n",
      "#213\n",
      "/layer3/layer3.1/conv1/Conv\n",
      "Conv((3, 3), 256 => 256, pad=1, bias=false)\n",
      "/layer3/layer3.1/bn1/BatchNormalization\n",
      "BatchNorm(256, relu)\n",
      "/layer3/layer3.1/conv2/Conv\n",
      "Conv((3, 3), 256 => 256, pad=1, bias=false)\n",
      "/layer3/layer3.1/bn2/BatchNormalization\n",
      "BatchNorm(256)\n",
      "/layer3/layer3.1/Add\n",
      "#341\n",
      "/layer3/layer3.1/relu_1/Relu\n",
      "#213\n",
      "/layer4/layer4.0/conv1/Conv\n",
      "Conv((3, 3), 256 => 512, pad=1, stride=2, bias=false)\n",
      "/layer4/layer4.0/bn1/BatchNormalization\n",
      "BatchNorm(512, relu)\n",
      "/layer4/layer4.0/conv2/Conv\n",
      "Conv((3, 3), 512 => 512, pad=1, bias=false)\n",
      "/layer4/layer4.0/bn2/BatchNormalization\n",
      "BatchNorm(512)\n",
      "/layer4/layer4.0/downsample/downsample.0/Conv\n",
      "Conv((1, 1), 256 => 512, stride=2, bias=false)\n",
      "/layer4/layer4.0/downsample/downsample.1/BatchNormalization\n",
      "BatchNorm(512)\n",
      "/layer4/layer4.0/Add\n",
      "#341\n",
      "/layer4/layer4.0/relu_1/Relu\n",
      "#213\n",
      "/layer4/layer4.1/conv1/Conv\n",
      "Conv((3, 3), 512 => 512, pad=1, bias=false)\n",
      "/layer4/layer4.1/bn1/BatchNormalization\n",
      "BatchNorm(512, relu)\n",
      "/layer4/layer4.1/conv2/Conv\n",
      "Conv((3, 3), 512 => 512, pad=1, bias=false)\n",
      "/layer4/layer4.1/bn2/BatchNormalization\n",
      "BatchNorm(512)\n",
      "/layer4/layer4.1/Add\n",
      "#341\n",
      "/layer4/layer4.1/relu_1/Relu\n",
      "#213\n",
      "/deconv_layers/deconv_layers.0/ConvTranspose\n",
      "ConvTranspose((2, 2), 512 => 256, stride=2, bias=false)\n",
      "/deconv_layers/deconv_layers.1/BatchNormalization\n",
      "BatchNorm(256, relu)\n",
      "/deconv_layers/deconv_layers.3/ConvTranspose\n",
      "ConvTranspose((2, 2), 256 => 128, stride=2, bias=false)\n",
      "/deconv_layers/deconv_layers.4/BatchNormalization\n",
      "BatchNorm(128, relu)\n",
      "/deconv_layers/deconv_layers.6/ConvTranspose\n",
      "ConvTranspose((2, 2), 128 => 64, stride=2, bias=false)\n",
      "/deconv_layers/deconv_layers.7/BatchNormalization\n",
      "BatchNorm(64, relu)\n",
      "/deconv_layers/deconv_layers.9/ConvTranspose\n",
      "ConvTranspose((2, 2), 64 => 64, stride=2, bias=false)\n",
      "/deconv_layers/deconv_layers.10/BatchNormalization\n",
      "BatchNorm(64, relu)\n",
      "/deconv_layers/deconv_layers.12/ConvTranspose\n",
      "ConvTranspose((2, 2), 64 => 64, stride=2, bias=false)\n",
      "/deconv_layers/deconv_layers.13/BatchNormalization\n",
      "BatchNorm(64, relu)\n",
      "/final_layer/Conv\n",
      "Conv((1, 1), 64 => 24)\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"/home/verification/ModelVerification.jl/onnx_parser/resnet_model.onnx\"\n",
    "comp_graph = ONNXNaiveNASflux.load(onnx_model_path, infer_shapes=false)\n",
    "\n",
    "for (index, vertex) in enumerate(vertices(comp_graph))\n",
    "    if index == 10\n",
    "        #println(layer(vertex))\n",
    "        #println(length(inputs(vertex)))\n",
    "        #println(outputs(vertex))\n",
    "        #println(NaiveNASflux.name(vertex))\n",
    "    end\n",
    "    println(NaiveNASflux.name(vertex))\n",
    "    println(layer(vertex))\n",
    "    #println(length(inputs(vertex)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set(Any[\"BatchNormalization\", \"Flatten\", \"Relu\", \"Conv\", \"Gemm\", \"AveragePool\", \"Add\"])\n",
      "true\n",
      "2\n",
      "+"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "typeof(+)\n"
     ]
    }
   ],
   "source": [
    "# +++++++++++++++++++ build flux model +++++++++++++++++++\n",
    "onnx_model_path = \"/home/verification/ModelVerification.jl/mlp.onnx\"\n",
    "#model = build_flux_model(onnx_model_path)\n",
    "#println.(model)\n",
    "queue = Queue{Any}()\n",
    "batch_info = Dict()\n",
    "global_info = Dict()\n",
    "comp_graph = ONNXNaiveNASflux.load(onnx_model_path)\n",
    "#= for (index, vertex) in enumerate(vertices(comp_graph))\n",
    "   # println(inputs(vertex), \"  \", outputs(vertex))\n",
    "   # println(layer(vertex))\n",
    "    if index < 2\n",
    "        continue\n",
    "    end \n",
    "    new_dict = Dict()\n",
    "    push!(new_dict, \"layer\" => layer(vertex))\n",
    "    push!(new_dict, \"index\" => index)\n",
    "    push!(new_dict, \"inputs\" => inputs(vertex))\n",
    "    push!(new_dict, \"outputs\" => outputs(vertex))\n",
    "    push!(batch_info, vertex => new_dict)\n",
    "    #= batch_info[index][\"layer\"] = layer(vertex)\n",
    "    batch_info[index][\"index\"] = index\n",
    "    batch_info[index][\"inputs\"] = inputs(vertex)\n",
    "    batch_info[index][\"outputs\"] = outputs(vertex) =#\n",
    "    enqueue!(queue, vertex)\n",
    "end =#\n",
    "#= layer_1 = outputs(dequeue!(queue))\n",
    "println(outputs(dequeue!(queue))) =#\n",
    "\n",
    "#= Vertex = []\n",
    "Isvisit = Dict()\n",
    "for (index, vertex) in enumerate(vertices(comp_graph))\n",
    "    # println(inputs(vertex), \"  \", outputs(vertex))\n",
    "    # println(layer(vertex))\n",
    "    push!(Vertex, vertex)\n",
    "    push!(Isvisit, vertex => false)\n",
    "end =#\n",
    "Vertex = []\n",
    "for (index, vertex) in enumerate(vertices(comp_graph))\n",
    "    if index < 2\n",
    "        push!(Vertex, nothing)\n",
    "        continue\n",
    "    end \n",
    "    if index == 4\n",
    "        #println(NaiveNASflux.name(vertex)) \n",
    "        #println(inputs(vertex))\n",
    "    end\n",
    "    push!(Vertex, Vertex)\n",
    "end\n",
    "\n",
    "\n",
    "for (index, vertex) in enumerate(vertices(comp_graph))\n",
    "    if index == 1 # the vertex which index == 1 has no useful information, so it's output node will be the start node of the model\n",
    "        push!(global_info, \"start_node\" => []) #creat a array for storing the start_node because mayer there are more than 1 start_node\n",
    "        for node in outputs(vertex)\n",
    "            push!(global_info[\"start_node\"], NaiveNASflux.name(node))\n",
    "        end\n",
    "        continue\n",
    "    end \n",
    "\n",
    "    new_dict = Dict() # store the information of this vertex \n",
    "    push!(new_dict, \"vertex\" => vertex)\n",
    "    push!(new_dict, \"layer\" => layer(vertex))\n",
    "    push!(new_dict, \"index\" => index)\n",
    "    push!(new_dict, \"inputs\" => inputs(vertex))\n",
    "    push!(new_dict, \"outputs\" => outputs(vertex))\n",
    "    push!(batch_info, NaiveNASflux.name(vertex) => new_dict) #new_dict belongs to batch_info\n",
    "    \n",
    "    if length(outputs(vertex)) == 0  #the final node has no output nodes\n",
    "        global_info[\"final_node\"] =  NaiveNASflux.name(vertex)\n",
    "    end\n",
    "    #println(layer(vertex))\n",
    "    #println(NaiveNASflux.name(vertex))\n",
    "    if index == 7\n",
    "        println(string(NaiveNASflux.name(vertex))[1:7] == \"Flatten\")\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "#output = batch_info[\"add_0\"][\"outputs\"]\n",
    "#println(typeof(output[1]))\n",
    "#println(batch_info[NaiveNASflux.name(output[1])])\n",
    "start_node = global_info[\"start_node\"]\n",
    "Isvisit = Dict()\n",
    "for node in start_node\n",
    "    enqueue!(queue, node)\n",
    "    batch_info[node][\"inputs\"] = nothing\n",
    "end\n",
    "\n",
    "for node in batch_info[\"conv_0\"][\"outputs\"]\n",
    "    #println(NaiveNASflux.name(node))\n",
    "    #println(batch_info[NaiveNASflux.name(node)])\n",
    "end\n",
    "\n",
    "#println(string(batch_info[\"add_0\"][\"layer\"]) == \"#341\")\n",
    "println(length(batch_info[\"add_0\"][\"inputs\"]))\n",
    "push!(batch_info[\"add_0\"], \"layer\" => +)\n",
    "println(batch_info[\"add_0\"][\"layer\"])\n",
    "println(typeof(batch_info[\"add_0\"][\"layer\"]))\n",
    "#println(batch_info[\"add_0\"][\"name\"])\n",
    "#= while isempty(queue)\n",
    "    node = dequeue!(queue)\n",
    "    if haskey(node)\n",
    "        continue\n",
    "    end\n",
    "    push!(Isvisit, node => true)\n",
    "    for layer in outputs(node)\n",
    "        if haskey(node)\n",
    "            continue\n",
    "        end\n",
    "        enqueue\n",
    "    end\n",
    "end     =# \n",
    "#= for (index, vertex) in enumerate(vertices(comp_graph))\n",
    "    if index < 2\n",
    "         continue\n",
    "    end \n",
    "    if index == 3\n",
    "        input = batch_info[vertex][\"inputs\"]\n",
    "        println(input)\n",
    "        println(haskey(batch_info, input))\n",
    "    end\n",
    "end  =#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function propagate(prop_method::ForwardProp, onnx_model_path, Flux_model, input_shape, batch_bound, batch_out_spec, aux_batch_info)\n",
    "    # input: batch x ... x ...\n",
    "\n",
    "    # dfs start from model.input_nodes\n",
    "    @assert !isnothing(onnx_model_path) \n",
    "\n",
    "    #= if !isnothing(Flux_model) && !isnothing(input_shape)\n",
    "        save(onnx_model_path, Flux_model, input_shape)\n",
    "    end =#\n",
    "\n",
    "    comp_graph = ONNXNaiveNASflux.load(onnx_model_path)\n",
    "    batch_info = Dict()\n",
    "    global_info = Dict()\n",
    "    for (index, vertex) in enumerate(ONNXNaiveNASflux.vertices(comp_graph))\n",
    "        if index == 1 # the vertex which index == 1 has no useful information, so it's output node will be the start node of the model\n",
    "            push!(global_info, \"start_node\" => []) #creat a array for storing the start_node because mayer there are more than 1 start_node\n",
    "            for node in outputs(vertex)\n",
    "                push!(global_info[\"start_node\"], NaiveNASflux.name(node))\n",
    "            end\n",
    "            continue\n",
    "        end \n",
    "    \n",
    "        new_dict = Dict() # store the information of this vertex \n",
    "        push!(new_dict, \"vertex\" => vertex)\n",
    "        push!(new_dict, \"layer\" => NaiveNASflux.layer(vertex))\n",
    "        push!(new_dict, \"index\" => index)\n",
    "        push!(new_dict, \"inputs\" => inputs(vertex))# note: inputs(vertex)) is not a string, use NaiveNASflux.name convert them to string \n",
    "        push!(new_dict, \"outputs\" => outputs(vertex))# note: outputs(vertex)) is not a string, use NaiveNASflux.name convert them to string \n",
    "        \n",
    "        if length(string(NaiveNASflux.name(vertex))) >= 7 && string(NaiveNASflux.name(vertex))[1:7] == \"Flatten\" \n",
    "            push!(new_dict, \"layer\" => Flux.flatten)\n",
    "        end\n",
    "\n",
    "        push!(batch_info, NaiveNASflux.name(vertex) => new_dict) #new_dict belongs to batch_info\n",
    "        if length(outputs(vertex)) == 0  #the final node has no output nodes\n",
    "            global_info[\"final_node\"] = NaiveNASflux.name(vertex)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    queue = Queue{Any}()\n",
    "    start_node = global_info[\"start_node\"]\n",
    "    Isvisit = Dict()\n",
    "    for node in start_node\n",
    "        enqueue!(queue, node)\n",
    "        batch_info[node][\"inputs\"] = nothing #start_nodes have no input nodes\n",
    "    end\n",
    "\n",
    "    while !isempty(queue)\n",
    "        node = dequeue!(queue)\n",
    "        if haskey(Isvisit, node) #means that this node has been visited\n",
    "            continue\n",
    "        end\n",
    "        push!(Isvisit, node => true)\n",
    "        for node in batch_info[node][\"outputs\"]\n",
    "            if haskey(Isvisit, node) #means that this node has been visited\n",
    "                continue\n",
    "            end\n",
    "            enqueue!(queue, NaiveNASflux.name(node))\n",
    "        end\n",
    "\n",
    "        if isnothing(batch_info[node][\"inputs\"])\n",
    "            current_batch_bound = batch_bound\n",
    "            batch_bound, aux_batch_info = forward_layer(prop_method, batch_info[node][\"layer\"], current_batch_bound, aux_batch_info)\n",
    "        elseif length(batch_info[node][\"inputs\"]) == 2\n",
    "            if string(batch_info[node][\"layer\"]) == \"#341\" # #342 means this node is an \"add\" layer\n",
    "                input_node1 = NaiveNASflux.name(batch_info[node][\"inputs\"][1])\n",
    "                input_node2 = NaiveNASflux.name(batch_info[node][\"inputs\"][2])\n",
    "                current_batch_bound1 = batch_info[input_node1][\"output_bound\"]\n",
    "                current_batch_bound2 = batch_info[input_node2][\"output_bound\"]\n",
    "                aux_batch_info1 = batch_info[input_node1][\"aux_batch_info\"]\n",
    "                aux_batch_info2 = batch_info[input_node2][\"aux_batch_info\"]\n",
    "                batch_bound, aux_batch_info = forward_skip_batch(prop_method, +, current_batch_bound1, current_batch_bound2, aux_batch_info1, aux_batch_info2)\n",
    "            end\n",
    "        else #length(batch_info[node][inputs] == 1\n",
    "            input_node = NaiveNASflux.name(batch_info[node][\"inputs\"][1])\n",
    "            current_batch_bound = batch_info[input_node][\"output_bound\"]\n",
    "            aux_batch_info = batch_info[input_node][\"aux_batch_info\"]\n",
    "            batch_bound, aux_batch_info = forward_layer(prop_method, batch_info[node][\"layer\"], current_batch_bound, aux_batch_info)\n",
    "        end\n",
    "        push!(batch_info[node], \"output_bound\" => batch_bound)\n",
    "        push!(batch_info[node], \"aux_batch_info\" => aux_batch_info)\n",
    "    end     \n",
    "\n",
    "    final_node = global_info[\"final_node\"]\n",
    "    batch_bound = batch_info[final_node][\"output_bound\"]\n",
    "    aux_batch_info = batch_info[final_node][\"aux_batch_info\"]\n",
    "\n",
    "    return batch_bound, aux_batch_info #need to change\n",
    "end    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
